#!/bin/bash

# Script para configurar sistemas RAID en Raspberry Pi
# Soporta BTRFS y ZFS

set -e

# Colores para output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# FunciÃ³n para mostrar mensajes
show_message() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

show_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

show_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

show_title() {
    echo -e "${BLUE}=== $1 ===${NC}"
}

# FunciÃ³n para obtener confirmaciÃ³n del usuario
confirm() {
    while true; do
        read -p "$1 (y/n): " yn
        case $yn in
            [Yy]* ) return 0;;
            [Nn]* ) return 1;;
            * ) echo "Por favor responde y o n.";;
        esac
    done
}

# FunciÃ³n para verificar e instalar requisitos del sistema
check_and_install_requirements() {
    show_title "Verificando Requisitos del Sistema"
    
    # Actualizar lista de paquetes
    show_message "Actualizando lista de paquetes..."
    sudo apt update > /dev/null 2>&1
    
    local packages_to_install=()
    local missing_packages=""
    
    # Verificar herramientas bÃ¡sicas
    if ! command -v lsblk &> /dev/null; then
        packages_to_install+=("util-linux")
        missing_packages="$missing_packages util-linux"
    fi
    
    if ! command -v parted &> /dev/null; then
        packages_to_install+=("parted")
        missing_packages="$missing_packages parted"
    fi
    
    if ! command -v wipefs &> /dev/null; then
        packages_to_install+=("util-linux")
        missing_packages="$missing_packages util-linux"
    fi
    
    # Verificar BTRFS
    local btrfs_available=false
    if command -v mkfs.btrfs &> /dev/null && command -v btrfs &> /dev/null; then
        btrfs_available=true
        show_message "âœ“ BTRFS: Disponible"
    else
        show_warning "âœ— BTRFS: No disponible"
        packages_to_install+=("btrfs-progs")
        missing_packages="$missing_packages btrfs-progs"
    fi
    
    # Verificar ZFS
    local zfs_available=false
    if command -v zpool &> /dev/null && command -v zfs &> /dev/null; then
        zfs_available=true
        show_message "âœ“ ZFS: Disponible"
    else
        show_warning "âœ— ZFS: No disponible"
        packages_to_install+=("zfsutils-linux")
        missing_packages="$missing_packages zfsutils-linux"
    fi
    
    # Verificar mdadm (Ãºtil para limpieza)
    if ! command -v mdadm &> /dev/null; then
        packages_to_install+=("mdadm")
        missing_packages="$missing_packages mdadm"
    fi
    
    # Si no hay herramientas RAID disponibles, no podemos continuar
    if [ "$btrfs_available" = false ] && [ "$zfs_available" = false ]; then
        show_error "No hay herramientas RAID disponibles en el sistema"
        show_message "Se instalarÃ¡n los paquetes necesarios..."
    fi
    
    # Instalar paquetes faltantes si es necesario
    if [ ${#packages_to_install[@]} -gt 0 ]; then
        show_message "Paquetes que se instalarÃ¡n:$missing_packages"
        
        # Verificar si ZFS estÃ¡ en la lista
        local installing_zfs=false
        for package in "${packages_to_install[@]}"; do
            if [ "$package" = "zfsutils-linux" ]; then
                installing_zfs=true
                break
            fi
        done
        
        if [ "$installing_zfs" = true ]; then
            show_warning "âš ï¸  ZFS puede tardar hasta 10 minutos en instalarse debido a la compilaciÃ³n de mÃ³dulos del kernel"
            show_warning "âš ï¸  Durante la instalaciÃ³n verÃ¡s el progreso en pantalla"
            
            if ! confirm "Â¿Deseas continuar con la instalaciÃ³n?"; then
                show_message "InstalaciÃ³n cancelada por el usuario"
                exit 0
            fi
        fi
        
        show_message "Iniciando instalaciÃ³n de paquetes..."
        echo "----------------------------------------"
        
        # Instalar paquetes con output visible
        for package in "${packages_to_install[@]}"; do
            if [ "$package" = "zfsutils-linux" ]; then
                show_message "ğŸ”„ Instalando ZFS (esto puede tomar varios minutos)..."
                echo "ğŸ“¦ Progreso de instalaciÃ³n ZFS:"
                if sudo apt install -y "$package"; then
                    show_message "âœ… ZFS instalado exitosamente"
                else
                    show_error "âŒ Error instalando ZFS"
                    exit 1
                fi
            else
                show_message "ğŸ”„ Instalando $package..."
                if sudo apt install -y "$package" > /dev/null 2>&1; then
                    show_message "âœ… $package instalado exitosamente"
                else
                    show_error "âŒ Error instalando $package"
                    exit 1
                fi
            fi
        done
        
        echo "----------------------------------------"
        show_message "âœ… Todos los paquetes se instalaron correctamente"
        
        # Verificar que ZFS estÃ© funcionando si se instalÃ³
        if [ "$installing_zfs" = true ]; then
            show_message "ğŸ”„ Verificando funcionamiento de ZFS..."
            
            # Cargar mÃ³dulo ZFS si no estÃ¡ cargado
            if ! lsmod | grep -q "^zfs "; then
                show_message "Cargando mÃ³dulo ZFS..."
                sudo modprobe zfs
                sleep 2
            fi
            
            # Verificar que los comandos funcionen
            if zpool status > /dev/null 2>&1 && zfs version > /dev/null 2>&1; then
                local zfs_version=$(zfs version | head -1 | awk '{print $2}')
                show_message "âœ… ZFS funcionando correctamente (versiÃ³n: $zfs_version)"
            else
                show_error "âŒ ZFS no estÃ¡ funcionando correctamente"
                show_message "Puede ser necesario reiniciar el sistema"
                if confirm "Â¿Deseas continuar de todas formas? (puede fallar)"; then
                    show_warning "Continuando con ZFS posiblemente no funcional..."
                else
                    exit 1
                fi
            fi
        fi
        
        # Breve pausa para que el usuario vea el resultado
        sleep 1
    else
        show_message "âœ… Todos los requisitos estÃ¡n disponibles"
    fi
    
    # Mostrar resumen final de herramientas disponibles
    show_message "Herramientas RAID disponibles:"
    if command -v mkfs.btrfs &> /dev/null; then
        local btrfs_version=$(btrfs --version 2>/dev/null | awk '{print $2}' || echo "desconocida")
        echo "  âœ“ BTRFS (versiÃ³n: $btrfs_version)"
    fi
    
    if command -v zpool &> /dev/null; then
        local zfs_version=$(zfs version 2>/dev/null | head -1 | awk '{print $2}' || echo "desconocida")
        echo "  âœ“ ZFS (versiÃ³n: $zfs_version)"
    fi
    
    echo ""
}

# FunciÃ³n para detectar todas las configuraciones RAID existentes
detect_existing_raid_configurations() {
    show_title "DetecciÃ³n de Configuraciones RAID Existentes"
    
    local zfs_found=false
    local btrfs_found=false
    local mdadm_found=false
    local lvm_found=false
    local any_raid_found=false
    
    # 1. DETECTAR ZFS
    if command -v zpool &> /dev/null; then
        # Verificar que el mÃ³dulo ZFS estÃ© cargado
        if ! lsmod | grep -q "^zfs "; then
            sudo modprobe zfs 2>/dev/null || true
            sleep 1
        fi
        
        local existing_pools=$(zpool list -H -o name 2>/dev/null)
        if [ -n "$existing_pools" ]; then
            zfs_found=true
            any_raid_found=true
            
            echo ""
            echo "ğŸ”· POOLS ZFS DETECTADOS:"
            for pool in $existing_pools; do
                local pool_health=$(zpool list -H -o health "$pool" 2>/dev/null)
                local pool_size=$(zpool list -H -o size "$pool" 2>/dev/null)
                local pool_used=$(zpool list -H -o allocated "$pool" 2>/dev/null)
                local pool_free=$(zpool list -H -o free "$pool" 2>/dev/null)
                
                echo "  ğŸ“¦ Pool: $pool"
                echo "     ğŸ’š Estado: $pool_health"
                echo "     ğŸ“ TamaÃ±o: $pool_size (Usado: $pool_used, Libre: $pool_free)"
                
                # Mostrar datasets existentes
                local datasets=$(zfs list -H -o name -r "$pool" 2>/dev/null | grep -v "^${pool}$")
                if [ -n "$datasets" ]; then
                    local dataset_count=$(echo "$datasets" | wc -l)
                    echo "     ğŸ“ Datasets: $dataset_count"
                    for dataset in $datasets; do
                        local used=$(zfs list -H -o used "$dataset" 2>/dev/null)
                        local mountpoint=$(zfs list -H -o mountpoint "$dataset" 2>/dev/null)
                        echo "       â€¢ $dataset (Usado: $used, Montaje: $mountpoint)"
                    done
                else
                    echo "     ğŸ“ Sin datasets (solo pool raÃ­z)"
                fi
                
                # Mostrar dispositivos del pool
                local pool_devices=$(zpool status "$pool" 2>/dev/null | grep -E "^\s+[a-z]" | awk '{print $1}' | grep -v "raidz\|mirror\|spare\|log\|cache\|replacing" | head -3)
                if [ -n "$pool_devices" ]; then
                    echo "     ğŸ’¿ Dispositivos: $(echo $pool_devices | tr '\n' ' ' | sed 's/ *$//')..."
                fi
                echo ""
            done
        fi
    fi
    
    # 2. DETECTAR BTRFS
    if command -v btrfs &> /dev/null; then
        local btrfs_devices=()
        local btrfs_info=()
        
        # Buscar en todos los dispositivos
        for device in $(lsblk -dpno NAME | grep -E "sd[a-z]|nvme"); do
            if btrfs filesystem show "$device" 2>/dev/null | grep -q "uuid:"; then
                local device_name=$(basename "$device")
                local btrfs_uuid=$(btrfs filesystem show "$device" 2>/dev/null | grep "uuid:" | awk '{print $4}')
                local size=$(lsblk -dpno SIZE "$device" | tr -d ' ')
                local mount_point=$(mount | grep "$device" | awk '{print $3}' | head -1)
                
                btrfs_devices+=("$device_name")
                if [ -n "$mount_point" ]; then
                    btrfs_info+=("$device_name:$size:$mount_point:$btrfs_uuid")
                else
                    btrfs_info+=("$device_name:$size:no_montado:$btrfs_uuid")
                fi
            fi
        done
        
        if [ ${#btrfs_devices[@]} -gt 0 ]; then
            btrfs_found=true
            any_raid_found=true
            
            echo ""
            echo "ğŸŸ  FILESYSTEMS BTRFS DETECTADOS:"
            for info in "${btrfs_info[@]}"; do
                IFS=':' read -r device size mount_point uuid <<< "$info"
                echo "  ğŸ“¦ Dispositivo: $device"
                echo "     ğŸ“ TamaÃ±o: $size"
                echo "     ğŸ“ Montaje: $mount_point"
                echo "     ğŸ†” UUID: $uuid"
                
                # Mostrar informaciÃ³n adicional de BTRFS si estÃ¡ montado
                if [ "$mount_point" != "no_montado" ]; then
                    local used=$(df -h "$mount_point" 2>/dev/null | tail -1 | awk '{print $3}')
                    local available=$(df -h "$mount_point" 2>/dev/null | tail -1 | awk '{print $4}')
                    if [ -n "$used" ]; then
                        echo "     ğŸ“Š Usado: $used, Disponible: $available"
                    fi
                    
                    # Verificar subvolÃºmenes
                    local subvolumes=$(btrfs subvolume list "$mount_point" 2>/dev/null | wc -l)
                    if [ "$subvolumes" -gt 0 ]; then
                        echo "     ğŸ“‚ SubvolÃºmenes: $subvolumes"
                    fi
                fi
                echo ""
            done
        fi
    fi
    
    # 3. DETECTAR MDADM (Linux Software RAID)
    if command -v mdadm &> /dev/null; then
        local mdadm_arrays=$(cat /proc/mdstat 2>/dev/null | grep "^md" | awk '{print $1}')
        if [ -n "$mdadm_arrays" ]; then
            mdadm_found=true
            any_raid_found=true
            
            echo ""
            echo "ğŸ”´ ARRAYS MDADM DETECTADOS:"
            for array in $mdadm_arrays; do
                local array_info=$(mdadm --detail "/dev/$array" 2>/dev/null)
                if [ -n "$array_info" ]; then
                    local raid_level=$(echo "$array_info" | grep "Raid Level" | awk '{print $4}')
                    local array_size=$(echo "$array_info" | grep "Array Size" | awk '{print $4$5}')
                    local state=$(echo "$array_info" | grep "State" | awk '{print $3}')
                    local num_devices=$(echo "$array_info" | grep "Total Devices" | awk '{print $4}')
                    
                    echo "  ğŸ“¦ Array: /dev/$array"
                    echo "     ğŸ”§ Nivel RAID: $raid_level"
                    echo "     ğŸ“ TamaÃ±o: $array_size"
                    echo "     ğŸ’š Estado: $state"
                    echo "     ğŸ’¿ Dispositivos: $num_devices"
                    
                    # Mostrar dispositivos del array
                    local devices=$(echo "$array_info" | grep "/dev/" | grep -v "failed\|spare" | awk '{print $7}' | head -3)
                    if [ -n "$devices" ]; then
                        echo "     ğŸ’¿ Miembros: $(echo $devices | tr '\n' ' ' | sed 's/ *$//')..."
                    fi
                    echo ""
                fi
            done
        fi
    fi
    
    # 4. DETECTAR LVM
    if command -v vgdisplay &> /dev/null; then
        local volume_groups=$(vgdisplay 2>/dev/null | grep "VG Name" | awk '{print $3}')
        if [ -n "$volume_groups" ]; then
            lvm_found=true
            any_raid_found=true
            
            echo ""
            echo "ğŸŸ£ VOLUME GROUPS LVM DETECTADOS:"
            for vg in $volume_groups; do
                local vg_info=$(vgdisplay "$vg" 2>/dev/null)
                if [ -n "$vg_info" ]; then
                    local vg_size=$(echo "$vg_info" | grep "VG Size" | awk '{print $3$4}')
                    local vg_free=$(echo "$vg_info" | grep "Free  PE" | awk '{print $5$6}')
                    local pv_count=$(echo "$vg_info" | grep "Cur PV" | awk '{print $3}')
                    local lv_count=$(echo "$vg_info" | grep "Cur LV" | awk '{print $3}')
                    
                    echo "  ğŸ“¦ Volume Group: $vg"
                    echo "     ğŸ“ TamaÃ±o: $vg_size"
                    echo "     ğŸ’¾ Libre: $vg_free"
                    echo "     ğŸ’¿ Physical Volumes: $pv_count"
                    echo "     ğŸ“ Logical Volumes: $lv_count"
                    
                    # Mostrar logical volumes
                    local logical_volumes=$(lvdisplay "$vg" 2>/dev/null | grep "LV Name" | awk '{print $3}' | head -3)
                    if [ -n "$logical_volumes" ]; then
                        echo "     ğŸ“‚ LVs: $(echo $logical_volumes | tr '\n' ' ' | sed 's/ *$//')..."
                    fi
                    echo ""
                fi
            done
        fi
    fi
    
    # MOSTRAR RESUMEN Y OPCIONES
    if [ "$any_raid_found" = true ]; then
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo ""
        echo "ğŸ› ï¸  OPCIONES DISPONIBLES:"
        
        local option_number=1
        
        if [ "$zfs_found" = true ]; then
            echo "   $option_number. Gestionar pools y datasets ZFS"
            zfs_option=$option_number
            ((option_number++))
            
            echo "   $option_number. Eliminar pools ZFS especÃ­ficos"
            zfs_delete_option=$option_number
            ((option_number++))
        fi
        
        if [ "$btrfs_found" = true ]; then
            echo "   $option_number. Eliminar filesystems BTRFS especÃ­ficos"
            btrfs_option=$option_number
            ((option_number++))
        fi
        
        if [ "$mdadm_found" = true ]; then
            echo "   $option_number. Gestionar arrays MDADM (informaciÃ³n solamente)"
            mdadm_option=$option_number
            ((option_number++))
        fi
        
        if [ "$lvm_found" = true ]; then
            echo "   $option_number. Gestionar Volume Groups LVM (informaciÃ³n solamente)"
            lvm_option=$option_number
            ((option_number++))
        fi
        
        echo "   $option_number. Continuar con configuraciÃ³n de nuevo RAID"
        continue_option=$option_number
        ((option_number++))
        
        echo "   $option_number. Salir del script"
        exit_option=$option_number
        echo ""
        
        while true; do
            read -p "ğŸ‘‰ Selecciona una opciÃ³n (1-$exit_option): " choice
            
            if [ "$zfs_found" = true ] && [ "$choice" = "$zfs_option" ]; then
                manage_existing_pools_datasets
                return 1  # Indica que se gestionaron configuraciones existentes
            elif [ "$zfs_found" = true ] && [ "$choice" = "$zfs_delete_option" ]; then
                delete_existing_zfs_pools
                return 1  # Indica que se eliminaron pools
            elif [ "$btrfs_found" = true ] && [ "$choice" = "$btrfs_option" ]; then
                local btrfs_devices_array=()
                for info in "${btrfs_info[@]}"; do
                    IFS=':' read -r device size mount_point uuid <<< "$info"
                    btrfs_devices_array+=("$device")
                done
                delete_existing_btrfs "${btrfs_devices_array[@]}"
                return 1  # Indica que se eliminaron filesystems
            elif [ "$mdadm_found" = true ] && [ "$choice" = "$mdadm_option" ]; then
                show_mdadm_details
                echo ""
                echo "Para gestionar arrays MDADM, usa herramientas especÃ­ficas como:"
                echo "  â€¢ mdadm --detail /dev/mdX"
                echo "  â€¢ mdadm --stop /dev/mdX"
                echo "  â€¢ mdadm --manage /dev/mdX --fail /dev/sdX"
                echo ""
                if confirm "Â¿Continuar con la detecciÃ³n de configuraciones?"; then
                    detect_existing_raid_configurations
                    return $?
                else
                    return 1
                fi
            elif [ "$lvm_found" = true ] && [ "$choice" = "$lvm_option" ]; then
                show_lvm_details
                echo ""
                echo "Para gestionar LVM, usa herramientas especÃ­ficas como:"
                echo "  â€¢ lvdisplay, vgdisplay, pvdisplay"
                echo "  â€¢ lvextend, lvreduce"
                echo "  â€¢ vgextend, vgreduce"
                echo ""
                if confirm "Â¿Continuar con la detecciÃ³n de configuraciones?"; then
                    detect_existing_raid_configurations
                    return $?
                else
                    return 1
                fi
            elif [ "$choice" = "$continue_option" ]; then
                show_message "Continuando con configuraciÃ³n de nuevo RAID..."
                return 0  # ContinÃºa con el flujo normal
            elif [ "$choice" = "$exit_option" ]; then
                show_message "Saliendo del script..."
                exit 0
            else
                echo "âŒ OpciÃ³n invÃ¡lida. Selecciona un nÃºmero vÃ¡lido."
            fi
        done
    else
        echo ""
        echo "â„¹ï¸  No se detectaron configuraciones RAID existentes."
        echo "   El sistema estÃ¡ listo para configurar un nuevo RAID."
        echo ""
        return 0  # ContinÃºa con configuraciÃ³n normal
    fi
}

# FunciÃ³n para mostrar detalles de arrays MDADM
show_mdadm_details() {
    show_title "Detalles de Arrays MDADM"
    
    local mdadm_arrays=$(cat /proc/mdstat 2>/dev/null | grep "^md" | awk '{print $1}')
    if [ -z "$mdadm_arrays" ]; then
        show_warning "No se encontraron arrays MDADM"
        return 0
    fi
    
    for array in $mdadm_arrays; do
        echo ""
        echo "ğŸ“¦ ARRAY: /dev/$array"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        
        # InformaciÃ³n detallada del array
        local array_info=$(mdadm --detail "/dev/$array" 2>/dev/null)
        if [ -n "$array_info" ]; then
            echo "$array_info"
        else
            echo "âŒ No se pudo obtener informaciÃ³n detallada del array"
        fi
        
        echo ""
        echo "ğŸ“Š ESTADO EN /proc/mdstat:"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        cat /proc/mdstat | grep -A 3 "^$array"
        echo ""
    done
}

# FunciÃ³n para mostrar detalles de Volume Groups LVM
show_lvm_details() {
    show_title "Detalles de Volume Groups LVM"
    
    local volume_groups=$(vgdisplay 2>/dev/null | grep "VG Name" | awk '{print $3}')
    if [ -z "$volume_groups" ]; then
        show_warning "No se encontraron Volume Groups LVM"
        return 0
    fi
    
    for vg in $volume_groups; do
        echo ""
        echo "ğŸ“¦ VOLUME GROUP: $vg"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        
        # InformaciÃ³n detallada del VG
        vgdisplay "$vg" 2>/dev/null
        
        echo ""
        echo "ğŸ“ LOGICAL VOLUMES EN $vg:"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        lvdisplay "$vg" 2>/dev/null
        
        echo ""
        echo "ğŸ’¿ PHYSICAL VOLUMES EN $vg:"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        pvdisplay 2>/dev/null | grep -A 10 "$vg" | head -15
        echo ""
    done
}

# FunciÃ³n para eliminar pools ZFS existentes
delete_existing_zfs_pools() {
    show_title "EliminaciÃ³n de Pools ZFS Existentes"
    
    local existing_pools=$(zpool list -H -o name 2>/dev/null)
    if [ -z "$existing_pools" ]; then
        show_warning "No se encontraron pools ZFS para eliminar"
        return 0
    fi
    
    local pools_array=($existing_pools)
    
    echo "âš ï¸  ADVERTENCIA: Esta operaciÃ³n eliminarÃ¡ completamente los pools seleccionados"
    echo "ğŸ”¥ TODOS LOS DATOS SE PERDERÃN PERMANENTEMENTE"
    echo ""
    
    echo "Pools ZFS disponibles para eliminar:"
    for i in "${!pools_array[@]}"; do
        local pool="${pools_array[$i]}"
        local pool_health=$(zpool list -H -o health "$pool" 2>/dev/null)
        local pool_size=$(zpool list -H -o size "$pool" 2>/dev/null)
        local pool_used=$(zpool list -H -o allocated "$pool" 2>/dev/null)
        
        echo "  $((i+1)). $pool"
        echo "     ğŸ’š Estado: $pool_health | ğŸ“Š TamaÃ±o: $pool_size | ğŸ“ˆ Usado: $pool_used"
        
        # Mostrar datasets
        local datasets=$(zfs list -H -o name -r "$pool" 2>/dev/null | grep -v "^${pool}$")
        if [ -n "$datasets" ]; then
            local dataset_count=$(echo "$datasets" | wc -l)
            echo "     ğŸ“ Datasets: $dataset_count"
        fi
    done
    
    echo ""
    echo "Opciones de eliminaciÃ³n:"
    echo "  â€¢ NÃºmero del pool (ej: 1, 2, 3)"
    echo "  â€¢ MÃºltiples pools separados por espacios (ej: 1 3 4)"
    echo "  â€¢ 'all' - Eliminar TODOS los pools (Â¡PELIGROSO!)"
    echo "  â€¢ 'cancel' - Cancelar operaciÃ³n"
    echo ""
    
    while true; do
        read -p "ğŸ‘‰ Pools a eliminar: " choice
        
        if [ "$choice" = "cancel" ]; then
            show_message "OperaciÃ³n cancelada por el usuario"
            return 0
        elif [ "$choice" = "all" ]; then
            show_warning "âš ï¸  Â¡ADVERTENCIA! Vas a eliminar TODOS los pools ZFS"
            show_warning "âš ï¸  Esto incluye: ${pools_array[*]}"
            
            if confirm "Â¿EstÃ¡s ABSOLUTAMENTE SEGURO de eliminar TODOS los pools?"; then
                for pool in "${pools_array[@]}"; do
                    echo ""
                    destroy_zfs_pool_safely "$pool"
                done
                show_message "âœ… Todos los pools han sido procesados"
                return 0
            else
                show_message "EliminaciÃ³n masiva cancelada"
                continue
            fi
        elif [[ "$choice" =~ ^[0-9\ ]+$ ]]; then
            # Validar selecciones
            local valid_selections=()
            local invalid_found=false
            
            for num in $choice; do
                if [[ "$num" =~ ^[0-9]+$ ]] && [ "$num" -ge 1 ] && [ "$num" -le ${#pools_array[@]} ]; then
                    local pool_index=$((num-1))
                    local pool_name="${pools_array[$pool_index]}"
                    if [[ ! " ${valid_selections[*]} " =~ " ${pool_name} " ]]; then
                        valid_selections+=("$pool_name")
                    fi
                else
                    echo "âŒ SelecciÃ³n invÃ¡lida: $num (usa nÃºmeros del 1 al ${#pools_array[@]})"
                    invalid_found=true
                fi
            done
            
            if [ "$invalid_found" = true ]; then
                continue
            fi
            
            if [ ${#valid_selections[@]} -eq 0 ]; then
                show_error "No se seleccionaron pools vÃ¡lidos"
                continue
            fi
            
            # Mostrar resumen de pools a eliminar
            echo ""
            show_warning "Pools seleccionados para eliminaciÃ³n:"
            for pool in "${valid_selections[@]}"; do
                local pool_size=$(zpool list -H -o size "$pool" 2>/dev/null)
                local pool_used=$(zpool list -H -o allocated "$pool" 2>/dev/null)
                echo "  ğŸ—‘ï¸  $pool (TamaÃ±o: $pool_size, Usado: $pool_used)"
            done
            
            echo ""
            if confirm "Â¿Confirmas la eliminaciÃ³n de estos ${#valid_selections[@]} pools?"; then
                for pool in "${valid_selections[@]}"; do
                    echo ""
                    destroy_zfs_pool_safely "$pool"
                done
                show_message "âœ… Pools seleccionados han sido procesados"
                return 0
            else
                show_message "EliminaciÃ³n cancelada"
                continue
            fi
        else
            echo "âŒ OpciÃ³n invÃ¡lida. Usa nÃºmeros, 'all' o 'cancel'"
        fi
    done
}

# FunciÃ³n para detectar y gestionar filesystems BTRFS existentes
detect_and_manage_btrfs() {
    if ! command -v btrfs &> /dev/null; then
        return 0
    fi
    
    # Buscar filesystems BTRFS existentes
    local btrfs_devices=()
    local btrfs_info=()
    
    # Buscar en todos los dispositivos
    for device in $(lsblk -dpno NAME | grep -E "sd[a-z]|nvme"); do
        if btrfs filesystem show "$device" 2>/dev/null | grep -q "uuid:"; then
            local device_name=$(basename "$device")
            local btrfs_uuid=$(btrfs filesystem show "$device" 2>/dev/null | grep "uuid:" | awk '{print $4}')
            local size=$(lsblk -dpno SIZE "$device" | tr -d ' ')
            local mount_point=$(mount | grep "$device" | awk '{print $3}' | head -1)
            
            btrfs_devices+=("$device_name")
            if [ -n "$mount_point" ]; then
                btrfs_info+=("$device_name:$size:$mount_point:$btrfs_uuid")
            else
                btrfs_info+=("$device_name:$size:no_montado:$btrfs_uuid")
            fi
        fi
    done
    
    if [ ${#btrfs_devices[@]} -gt 0 ]; then
        show_title "Filesystems BTRFS Existentes Detectados"
        echo ""
        echo "Se encontraron los siguientes filesystems BTRFS:"
        
        for info in "${btrfs_info[@]}"; do
            IFS=':' read -r device size mount_point uuid <<< "$info"
            echo "  ğŸ“¦ Dispositivo: $device"
            echo "     ğŸ“ TamaÃ±o: $size"
            echo "     ğŸ“ Montaje: $mount_point"
            echo "     ğŸ†” UUID: $uuid"
            echo ""
        done
        
        echo "ğŸ› ï¸  OPCIONES PARA BTRFS:"
        echo "   1. Eliminar filesystems BTRFS especÃ­ficos"
        echo "   2. Continuar sin modificar BTRFS"
        echo ""
        
        while true; do
            read -p "ğŸ‘‰ Selecciona una opciÃ³n (1-2): " choice
            case $choice in
                1)
                    delete_existing_btrfs "${btrfs_devices[@]}"
                    break
                    ;;
                2)
                    show_message "Continuando sin modificar filesystems BTRFS..."
                    break
                    ;;
                *)
                    echo "âŒ OpciÃ³n invÃ¡lida. Selecciona 1 o 2."
                    ;;
            esac
        done
    fi
    
    return 0
}

# FunciÃ³n para eliminar filesystems BTRFS existentes
delete_existing_btrfs() {
    local btrfs_devices=("$@")
    
    show_title "EliminaciÃ³n de Filesystems BTRFS"
    
    echo "âš ï¸  ADVERTENCIA: Esta operaciÃ³n eliminarÃ¡ completamente los filesystems seleccionados"
    echo "ğŸ”¥ TODOS LOS DATOS SE PERDERÃN PERMANENTEMENTE"
    echo ""
    
    echo "Filesystems BTRFS disponibles para eliminar:"
    for i in "${!btrfs_devices[@]}"; do
        local device="${btrfs_devices[$i]}"
        local size=$(lsblk -dpno SIZE "/dev/$device" | tr -d ' ')
        local mount_point=$(mount | grep "/dev/$device" | awk '{print $3}' | head -1)
        local uuid=$(btrfs filesystem show "/dev/$device" 2>/dev/null | grep "uuid:" | awk '{print $4}')
        
        echo "  $((i+1)). $device"
        echo "     ğŸ“ TamaÃ±o: $size"
        if [ -n "$mount_point" ]; then
            echo "     ğŸ“ Montado en: $mount_point"
        else
            echo "     ğŸ“ No montado"
        fi
        echo "     ğŸ†” UUID: $uuid"
    done
    
    echo ""
    echo "Opciones de eliminaciÃ³n:"
    echo "  â€¢ NÃºmero del dispositivo (ej: 1, 2, 3)"
    echo "  â€¢ MÃºltiples dispositivos separados por espacios (ej: 1 3)"
    echo "  â€¢ 'all' - Eliminar TODOS los filesystems BTRFS"
    echo "  â€¢ 'cancel' - Cancelar operaciÃ³n"
    echo ""
    
    while true; do
        read -p "ğŸ‘‰ Filesystems a eliminar: " choice
        
        if [ "$choice" = "cancel" ]; then
            show_message "OperaciÃ³n cancelada por el usuario"
            return 0
        elif [ "$choice" = "all" ]; then
            show_warning "âš ï¸  Â¡ADVERTENCIA! Vas a eliminar TODOS los filesystems BTRFS"
            show_warning "âš ï¸  Esto incluye: ${btrfs_devices[*]}"
            
            if confirm "Â¿EstÃ¡s ABSOLUTAMENTE SEGURO de eliminar TODOS los filesystems BTRFS?"; then
                # Agrupar dispositivos por UUID para manejar arrays multi-disco
                local processed_uuids=()
                
                for device in "${btrfs_devices[@]}"; do
                    local device_uuid=$(sudo btrfs filesystem show "/dev/$device" 2>/dev/null | grep "uuid:" | awk '{print $4}')
                    
                    # Verificar si ya procesamos este UUID
                    if [[ ! " ${processed_uuids[*]} " =~ " ${device_uuid} " ]]; then
                        processed_uuids+=("$device_uuid")
                        
                        # Obtener todos los dispositivos del array con este UUID
                        local array_devices=$(sudo btrfs filesystem show "/dev/$device" 2>/dev/null | grep "devid" | awk '{print $NF}' | sed 's|/dev/||')
                        
                        echo ""
                        show_warning "ğŸ—‘ï¸  Eliminando array BTRFS (UUID: $device_uuid)"
                        show_message "ğŸ“€ Dispositivos del array: $array_devices"
                        
                        # Usar el primer dispositivo para la eliminaciÃ³n (que limpiarÃ¡ todo el array)
                        destroy_btrfs_array_safely "$device" "$device_uuid"
                    fi
                done
                show_message "âœ… Todos los filesystems BTRFS han sido procesados"
                return 0
            else
                show_message "EliminaciÃ³n masiva cancelada"
                continue
            fi
        elif [[ "$choice" =~ ^[0-9\ ]+$ ]]; then
            # Validar selecciones
            local valid_selections=()
            local invalid_found=false
            
            for num in $choice; do
                if [[ "$num" =~ ^[0-9]+$ ]] && [ "$num" -ge 1 ] && [ "$num" -le ${#btrfs_devices[@]} ]; then
                    local device_index=$((num-1))
                    local device_name="${btrfs_devices[$device_index]}"
                    if [[ ! " ${valid_selections[*]} " =~ " ${device_name} " ]]; then
                        valid_selections+=("$device_name")
                    fi
                else
                    echo "âŒ SelecciÃ³n invÃ¡lida: $num (usa nÃºmeros del 1 al ${#btrfs_devices[@]})"
                    invalid_found=true
                fi
            done
            
            if [ "$invalid_found" = true ]; then
                continue
            fi
            
            if [ ${#valid_selections[@]} -eq 0 ]; then
                show_error "No se seleccionaron dispositivos vÃ¡lidos"
                continue
            fi
            
            # Mostrar resumen de dispositivos a eliminar
            echo ""
            show_warning "Filesystems BTRFS seleccionados para eliminaciÃ³n:"
            for device in "${valid_selections[@]}"; do
                local size=$(lsblk -dpno SIZE "/dev/$device" | tr -d ' ')
                echo "  ğŸ—‘ï¸  $device (TamaÃ±o: $size)"
            done
            
            echo ""
            if confirm "Â¿Confirmas la eliminaciÃ³n de estos ${#valid_selections[@]} filesystems?"; then
                # Agrupar dispositivos por UUID para manejar arrays multi-disco
                local processed_uuids=()
                
                for device in "${valid_selections[@]}"; do
                    local device_uuid=$(sudo btrfs filesystem show "/dev/$device" 2>/dev/null | grep "uuid:" | awk '{print $4}')
                    
                    # Verificar si ya procesamos este UUID
                    if [[ ! " ${processed_uuids[*]} " =~ " ${device_uuid} " ]]; then
                        processed_uuids+=("$device_uuid")
                        
                        # Obtener informaciÃ³n del array
                        local btrfs_info=$(sudo btrfs filesystem show "/dev/$device" 2>/dev/null)
                        local total_devices=$(echo "$btrfs_info" | grep "Total devices" | awk '{print $3}')
                        
                        echo ""
                        if [ "$total_devices" -gt 1 ]; then
                            # Es un array multi-disco, usar la funciÃ³n de array
                            destroy_btrfs_array_safely "$device" "$device_uuid"
                        else
                            # Es un disco individual, usar la funciÃ³n tradicional
                            destroy_btrfs_safely "$device"
                        fi
                    fi
                done
                show_message "âœ… Filesystems BTRFS seleccionados han sido procesados"
                return 0
            else
                show_message "EliminaciÃ³n cancelada"
                continue
            fi
        else
            echo "âŒ OpciÃ³n invÃ¡lida. Usa nÃºmeros, 'all' o 'cancel'"
        fi
    done
}

# FunciÃ³n para gestionar datasets de pools existentes
manage_existing_pools_datasets() {
    show_title "GestiÃ³n de Datasets en Pools Existentes"
    
    local existing_pools=$(zpool list -H -o name 2>/dev/null)
    local pools_array=($existing_pools)
    
    echo "Selecciona el pool donde quieres gestionar datasets:"
    for i in "${!pools_array[@]}"; do
        local pool="${pools_array[$i]}"
        local pool_health=$(zpool list -H -o health "$pool" 2>/dev/null)
        local pool_free=$(zpool list -H -o free "$pool" 2>/dev/null)
        echo "  $((i+1)). $pool (Estado: $pool_health, Libre: $pool_free)"
    done
    echo ""
    
    while true; do
        read -p "ğŸ‘‰ Selecciona pool (1-${#pools_array[@]}): " pool_choice
        if [[ "$pool_choice" =~ ^[0-9]+$ ]] && [ "$pool_choice" -ge 1 ] && [ "$pool_choice" -le ${#pools_array[@]} ]; then
            local selected_pool="${pools_array[$((pool_choice-1))]}"
            create_datasets_in_pool "$selected_pool"
            break
        else
            echo "âŒ SelecciÃ³n invÃ¡lida. Usa nÃºmeros del 1 al ${#pools_array[@]}."
        fi
    done
    
    # Preguntar si quiere gestionar otro pool
    if [ ${#pools_array[@]} -gt 1 ]; then
        if confirm "Â¿Deseas gestionar datasets en otro pool?"; then
            manage_existing_pools_datasets
        fi
    fi
}

# FunciÃ³n para eliminar datasets de un pool especÃ­fico
delete_dataset_from_pool() {
    local pool_name="$1"
    
    show_title "EliminaciÃ³n de Datasets en Pool '$pool_name'"
    
    # Obtener datasets existentes (excluyendo el pool raÃ­z)
    local existing_datasets=$(zfs list -H -o name -r "$pool_name" 2>/dev/null | grep -v "^${pool_name}$")
    
    if [ -z "$existing_datasets" ]; then
        show_warning "No hay datasets para eliminar en el pool '$pool_name'"
        echo "Solo existe el pool raÃ­z que no puede ser eliminado desde aquÃ­."
        return 0
    fi
    
    # Convertir a array para manejo mÃ¡s fÃ¡cil
    local datasets_array=()
    while IFS= read -r dataset; do
        datasets_array+=("$dataset")
    done <<< "$existing_datasets"
    
    echo ""
    echo "âš ï¸  ADVERTENCIA: La eliminaciÃ³n de datasets es PERMANENTE"
    echo "ğŸ”¥ TODOS LOS DATOS del dataset seleccionado se perderÃ¡n"
    echo ""
    echo "ğŸ“ Datasets disponibles para eliminar:"
    
    for i in "${!datasets_array[@]}"; do
        local dataset="${datasets_array[$i]}"
        local used=$(zfs list -H -o used "$dataset" 2>/dev/null)
        local mountpoint=$(zfs list -H -o mountpoint "$dataset" 2>/dev/null)
        local compression=$(zfs get -H -o value compression "$dataset" 2>/dev/null)
        
        echo "  $((i+1)). $dataset"
        echo "     ğŸ“Š Usado: $used | ğŸ“ Montaje: $mountpoint | ğŸ—œï¸ CompresiÃ³n: $compression"
        
        # Mostrar snapshots si existen
        local snapshots=$(zfs list -t snapshot -H -o name "$dataset" 2>/dev/null | wc -l)
        if [ "$snapshots" -gt 0 ]; then
            echo "     ğŸ“¸ Snapshots: $snapshots (tambiÃ©n serÃ¡n eliminados)"
        fi
        echo ""
    done
    
    echo "Opciones de eliminaciÃ³n:"
    echo "  â€¢ NÃºmero del dataset (ej: 1, 2, 3)"
    echo "  â€¢ MÃºltiples datasets separados por espacios (ej: 1 3 4)"
    echo "  â€¢ 'all' - Eliminar TODOS los datasets (Â¡PELIGROSO!)"
    echo "  â€¢ 'cancel' - Cancelar operaciÃ³n"
    echo ""
    
    while true; do
        read -p "ğŸ‘‰ Datasets a eliminar: " choice
        
        if [ "$choice" = "cancel" ]; then
            show_message "EliminaciÃ³n cancelada por el usuario"
            return 0
        elif [ "$choice" = "all" ]; then
            show_warning "âš ï¸  Â¡ADVERTENCIA! Vas a eliminar TODOS los datasets"
            show_warning "âš ï¸  Esto incluye: ${datasets_array[*]}"
            echo ""
            echo "ğŸ”¥ ESTO ELIMINARÃ PERMANENTEMENTE:"
            for dataset in "${datasets_array[@]}"; do
                local used=$(zfs list -H -o used "$dataset" 2>/dev/null)
                echo "   â€¢ $dataset (Usado: $used)"
            done
            echo ""
            
            if confirm "Â¿EstÃ¡s ABSOLUTAMENTE SEGURO de eliminar TODOS los datasets?"; then
                # Eliminar en orden inverso para manejar dependencias
                for ((i=${#datasets_array[@]}-1; i>=0; i--)); do
                    local dataset="${datasets_array[$i]}"
                    echo ""
                    delete_dataset_safely "$dataset"
                done
                show_message "âœ… Todos los datasets han sido procesados"
                return 0
            else
                show_message "EliminaciÃ³n masiva cancelada"
                continue
            fi
        elif [[ "$choice" =~ ^[0-9\ ]+$ ]]; then
            # Validar selecciones
            local valid_selections=()
            local invalid_found=false
            
            for num in $choice; do
                if [[ "$num" =~ ^[0-9]+$ ]] && [ "$num" -ge 1 ] && [ "$num" -le ${#datasets_array[@]} ]; then
                    local dataset_index=$((num-1))
                    local dataset_name="${datasets_array[$dataset_index]}"
                    if [[ ! " ${valid_selections[*]} " =~ " ${dataset_name} " ]]; then
                        valid_selections+=("$dataset_name")
                    fi
                else
                    echo "âŒ SelecciÃ³n invÃ¡lida: $num (usa nÃºmeros del 1 al ${#datasets_array[@]})"
                    invalid_found=true
                fi
            done
            
            if [ "$invalid_found" = true ]; then
                continue
            fi
            
            if [ ${#valid_selections[@]} -eq 0 ]; then
                show_error "No se seleccionaron datasets vÃ¡lidos"
                continue
            fi
            
            # Mostrar resumen de datasets a eliminar
            echo ""
            show_warning "Datasets seleccionados para eliminaciÃ³n:"
            for dataset in "${valid_selections[@]}"; do
                local used=$(zfs list -H -o used "$dataset" 2>/dev/null)
                local snapshots=$(zfs list -t snapshot -H -o name "$dataset" 2>/dev/null | wc -l)
                echo "  ğŸ—‘ï¸  $dataset (Usado: $used)"
                if [ "$snapshots" -gt 0 ]; then
                    echo "      ğŸ“¸ Incluye $snapshots snapshots"
                fi
            done
            
            echo ""
            if confirm "Â¿Confirmas la eliminaciÃ³n de estos ${#valid_selections[@]} datasets?"; then
                # Ordenar datasets para eliminar los mÃ¡s profundos primero
                # (importante para evitar errores de dependencias)
                local sorted_selections=($(printf '%s\n' "${valid_selections[@]}" | sort -r))
                
                for dataset in "${sorted_selections[@]}"; do
                    echo ""
                    delete_dataset_safely "$dataset"
                done
                show_message "âœ… Datasets seleccionados han sido procesados"
                return 0
            else
                show_message "EliminaciÃ³n cancelada"
                continue
            fi
        else
            echo "âŒ OpciÃ³n invÃ¡lida. Usa nÃºmeros, 'all' o 'cancel'"
        fi
    done
}

# FunciÃ³n para eliminar un dataset de forma segura
delete_dataset_safely() {
    local dataset="$1"
    
    if [ -z "$dataset" ]; then
        show_error "Nombre de dataset no especificado"
        return 1
    fi
    
    # Verificar que el dataset existe
    if ! zfs list "$dataset" >/dev/null 2>&1; then
        show_warning "Dataset '$dataset' no encontrado"
        return 0
    fi
    
    show_warning "ğŸ—‘ï¸  Eliminando dataset: '$dataset'"
    
    # Mostrar informaciÃ³n del dataset antes de eliminar
    local used=$(zfs list -H -o used "$dataset" 2>/dev/null)
    local mountpoint=$(zfs list -H -o mountpoint "$dataset" 2>/dev/null)
    local compression=$(zfs get -H -o value compression "$dataset" 2>/dev/null)
    
    echo "   ğŸ“Š Datos a eliminar: $used"
    echo "   ğŸ“ Punto de montaje: $mountpoint"
    echo "   ğŸ—œï¸  CompresiÃ³n: $compression"
    
    # Verificar y mostrar snapshots
    local snapshots=$(zfs list -t snapshot -H -o name "$dataset" 2>/dev/null)
    if [ -n "$snapshots" ]; then
        local snapshot_count=$(echo "$snapshots" | wc -l)
        echo "   ğŸ“¸ Snapshots a eliminar: $snapshot_count"
        for snapshot in $snapshots; do
            local snap_used=$(zfs list -t snapshot -H -o used "$snapshot" 2>/dev/null)
            echo "      â€¢ $snapshot (Usado: $snap_used)"
        done
    fi
    
    # Verificar si tiene datasets hijos
    local child_datasets=$(zfs list -H -o name -r "$dataset" 2>/dev/null | grep -v "^${dataset}$")
    if [ -n "$child_datasets" ]; then
        echo "   ğŸ“‚ Datasets hijos que tambiÃ©n serÃ¡n eliminados:"
        for child in $child_datasets; do
            local child_used=$(zfs list -H -o used "$child" 2>/dev/null)
            echo "      â€¢ $child (Usado: $child_used)"
        done
    fi
    
    echo ""
    show_warning "âš ï¸  ESTA ACCIÃ“N ELIMINARÃ PERMANENTEMENTE:"
    show_warning "    â€¢ El dataset '$dataset' y todos sus datos"
    if [ -n "$snapshots" ]; then
        show_warning "    â€¢ Todos los snapshots del dataset"
    fi
    if [ -n "$child_datasets" ]; then
        show_warning "    â€¢ Todos los datasets hijos y sus datos"
    fi
    echo ""
    
    # Proceder con la eliminaciÃ³n
    show_message "ğŸ”„ Eliminando dataset '$dataset'..."
    
    # 1. Desmontar el dataset si estÃ¡ montado
    if [ "$mountpoint" != "none" ] && [ "$mountpoint" != "-" ]; then
        show_message "Desmontando dataset..."
        sudo zfs unmount "$dataset" 2>/dev/null || true
    fi
    
    # 2. Eliminar el dataset con todos sus snapshots y descendientes
    show_message "Destruyendo dataset y dependencias..."
    if sudo zfs destroy -r "$dataset" 2>/dev/null; then
        show_message "âœ… Dataset '$dataset' eliminado exitosamente"
    else
        show_warning "âš ï¸  Intento estÃ¡ndar fallÃ³, intentando eliminaciÃ³n forzada..."
        
        # Intentar eliminaciÃ³n forzada
        if sudo zfs destroy -f -r "$dataset" 2>/dev/null; then
            show_message "âœ… Dataset '$dataset' eliminado forzadamente"
        else
            show_error "âŒ No se pudo eliminar el dataset '$dataset'"
            show_error "    Posibles causas:"
            show_error "    â€¢ Dataset en uso por algÃºn proceso"
            show_error "    â€¢ Dependencias de snapshots o clones"
            show_error "    â€¢ Permisos insuficientes"
            return 1
        fi
    fi
    
    return 0
}

# FunciÃ³n para crear datasets en un pool especÃ­fico (reutilizable)
create_datasets_in_pool() {
    local pool_name="$1"
    
    show_title "GestiÃ³n de Datasets en Pool '$pool_name'"
    echo ""
    echo "ğŸ’¡ INFORMACIÃ“N SOBRE DATASETS:"
    echo "   Los datasets son subdivisiones lÃ³gicas dentro del pool ZFS."
    echo "   Beneficios:"
    echo "   â€¢ OrganizaciÃ³n de datos (data, backups, media, etc.)"
    echo "   â€¢ Cuotas de espacio independientes"
    echo "   â€¢ Snapshots y backups granulares"
    echo "   â€¢ Diferentes configuraciones de compresiÃ³n"
    echo "   â€¢ Puntos de montaje personalizados"
    echo ""
    echo "   Ejemplo: $pool_name/data, $pool_name/backups, $pool_name/media"
    echo ""
    
    # Mostrar datasets existentes
    local existing_datasets=$(zfs list -H -o name -r "$pool_name" 2>/dev/null | grep -v "^${pool_name}$")
    if [ -n "$existing_datasets" ]; then
        echo "ğŸ“ Datasets existentes en '$pool_name':"
        for dataset in $existing_datasets; do
            local used=$(zfs list -H -o used "$dataset" 2>/dev/null)
            local mountpoint=$(zfs list -H -o mountpoint "$dataset" 2>/dev/null)
            local compression=$(zfs get -H -o value compression "$dataset" 2>/dev/null)
            echo "  âœ“ $dataset"
            echo "     ğŸ“Š Usado: $used | ğŸ“ Montaje: $mountpoint | ğŸ—œï¸ CompresiÃ³n: $compression"
        done
        echo ""
    fi
    
    echo "ğŸ“‹ DATASETS COMUNES SUGERIDOS:"
    echo "   1. data     - Datos principales y documentos"
    echo "   2. media    - Videos, fotos, mÃºsica"
    echo "   3. backups  - Copias de seguridad"
    echo "   4. temp     - Archivos temporales"
    echo "   5. vm       - MÃ¡quinas virtuales"
    echo "   6. docker   - Contenedores Docker"
    echo "   7. homes    - Directorios de usuario"
    echo ""
    
    local datasets_created=()
    
    while true; do
        echo "Opciones:"
        echo "  â€¢ Nombre del dataset (ej: data, media, backups)"
        echo "  â€¢ 'suggested' - Crear datasets comunes (data, media, backups)"
        echo "  â€¢ 'list' - Mostrar datasets actuales"
        echo "  â€¢ 'delete' - Eliminar dataset existente"
        echo "  â€¢ 'done' - Finalizar gestiÃ³n de datasets"
        echo ""
        
        read -p "ğŸ‘‰ Dataset a crear/gestionar: " dataset_choice
        
        if [ "$dataset_choice" = "done" ]; then
            break
        elif [ "$dataset_choice" = "list" ]; then
            # Mostrar datasets actuales
            local current_datasets=$(zfs list -H -o name -r "$pool_name" 2>/dev/null | grep -v "^${pool_name}$")
            if [ -n "$current_datasets" ]; then
                echo ""
                echo "ğŸ“ Datasets actuales en '$pool_name':"
                for dataset in $current_datasets; do
                    local used=$(zfs list -H -o used "$dataset" 2>/dev/null)
                    local avail=$(zfs list -H -o avail "$dataset" 2>/dev/null)
                    local mountpoint=$(zfs list -H -o mountpoint "$dataset" 2>/dev/null)
                    echo "  âœ“ $dataset (Usado: $used, Disponible: $avail, Montaje: $mountpoint)"
                done
                echo ""
            else
                echo "  (no hay datasets creados aÃºn)"
                echo ""
            fi
        elif [ "$dataset_choice" = "delete" ]; then
            delete_dataset_from_pool "$pool_name"
        elif [ "$dataset_choice" = "suggested" ]; then
            local suggested_datasets=("data" "media" "backups")
            
            show_message "Creando datasets sugeridos: ${suggested_datasets[*]}"
            
            for dataset in "${suggested_datasets[@]}"; do
                # Verificar si ya existe
                if zfs list "$pool_name/$dataset" >/dev/null 2>&1; then
                    show_message "â„¹ï¸  Dataset '$pool_name/$dataset' ya existe"
                    continue
                fi
                
                show_message "Creando dataset: $pool_name/$dataset"
                
                if sudo zfs create "$pool_name/$dataset"; then
                    datasets_created+=("$dataset")
                    show_message "âœ… Dataset '$pool_name/$dataset' creado exitosamente"
                    
                    # Configurar montaje automÃ¡tico
                    sudo zfs set mountpoint="/$pool_name/$dataset" "$pool_name/$dataset"
                    
                    # Configuraciones especÃ­ficas por tipo
                    case $dataset in
                        "media")
                            # Optimizar para archivos grandes
                            sudo zfs set recordsize=1M "$pool_name/$dataset"
                            sudo zfs set compression=lz4 "$pool_name/$dataset"
                            show_message "  ğŸ“º Optimizado para archivos multimedia"
                            ;;
                        "backups")
                            # MÃ¡xima compresiÃ³n para backups
                            sudo zfs set compression=gzip "$pool_name/$dataset"
                            sudo zfs set dedup=on "$pool_name/$dataset" 2>/dev/null || true
                            show_message "  ğŸ’¾ Optimizado para backups (alta compresiÃ³n)"
                            ;;
                        "data")
                            # Balance entre compresiÃ³n y rendimiento
                            sudo zfs set compression=lz4 "$pool_name/$dataset"
                            show_message "  ğŸ“ Optimizado para datos generales"
                            ;;
                    esac
                else
                    show_error "âŒ Error creando dataset '$pool_name/$dataset'"
                fi
            done
            
        elif [ -n "$dataset_choice" ] && [[ "$dataset_choice" =~ ^[a-zA-Z0-9_-]+$ ]]; then
            # Crear dataset individual
            if zfs list "$pool_name/$dataset_choice" >/dev/null 2>&1; then
                show_message "â„¹ï¸  Dataset '$pool_name/$dataset_choice' ya existe"
                continue
            fi
            
            show_message "Creando dataset: $pool_name/$dataset_choice"
            
            if sudo zfs create "$pool_name/$dataset_choice"; then
                datasets_created+=("$dataset_choice")
                show_message "âœ… Dataset '$pool_name/$dataset_choice' creado exitosamente"
                
                # Configurar montaje
                sudo zfs set mountpoint="/$pool_name/$dataset_choice" "$pool_name/$dataset_choice"
                
                # Preguntar por configuraciones especÃ­ficas
                echo ""
                echo "âš™ï¸  CONFIGURACIONES OPCIONALES para '$dataset_choice':"
                
                if confirm "Â¿Configurar compresiÃ³n? (recomendado)"; then
                    echo "Tipos de compresiÃ³n:"
                    echo "1. lz4 - RÃ¡pida, buen balance (recomendado)"
                    echo "2. gzip - Alta compresiÃ³n, mÃ¡s lenta"
                    echo "3. off - Sin compresiÃ³n"
                    
                    read -p "Selecciona (1-3) [1]: " compression_choice
                    compression_choice=${compression_choice:-1}
                    
                    case $compression_choice in
                        1) sudo zfs set compression=lz4 "$pool_name/$dataset_choice";;
                        2) sudo zfs set compression=gzip "$pool_name/$dataset_choice";;
                        3) sudo zfs set compression=off "$pool_name/$dataset_choice";;
                    esac
                    show_message "  ğŸ—œï¸  CompresiÃ³n configurada"
                fi
                
                if confirm "Â¿Establecer cuota de espacio?"; then
                    read -p "Cuota en GB (ej: 100): " quota_gb
                    if [[ "$quota_gb" =~ ^[0-9]+$ ]] && [ "$quota_gb" -gt 0 ]; then
                        sudo zfs set quota="${quota_gb}G" "$pool_name/$dataset_choice"
                        show_message "  ğŸ“ Cuota establecida: ${quota_gb}GB"
                    fi
                fi
                
            else
                show_error "âŒ Error creando dataset '$pool_name/$dataset_choice'"
            fi
            
        else
            if [ -z "$dataset_choice" ]; then
                show_error "âŒ El nombre del dataset no puede estar vacÃ­o"
            else
                show_error "âŒ Nombre invÃ¡lido. Usa solo letras, nÃºmeros, guiones y guiones bajos"
            fi
        fi
        
        echo ""
    done
    
    # Mostrar resumen de datasets creados en esta sesiÃ³n
    if [ ${#datasets_created[@]} -gt 0 ]; then
        echo ""
        show_message "ğŸ“Š RESUMEN DE DATASETS CREADOS EN ESTA SESIÃ“N:"
        for dataset in "${datasets_created[@]}"; do
            local mountpoint=$(zfs get -H -o value mountpoint "$pool_name/$dataset" 2>/dev/null)
            local compression=$(zfs get -H -o value compression "$pool_name/$dataset" 2>/dev/null)
            local quota=$(zfs get -H -o value quota "$pool_name/$dataset" 2>/dev/null)
            
            echo "  âœ… $pool_name/$dataset"
            echo "     ğŸ“ Montaje: $mountpoint"
            echo "     ğŸ—œï¸  CompresiÃ³n: $compression"
            if [ "$quota" != "none" ]; then
                echo "     ğŸ“ Cuota: $quota"
            fi
            echo ""
        done
    fi
}

# FunciÃ³n para verificar si un disco estÃ¡ en uso por RAID
check_disk_raid_status() {
    local disk="$1"
    local raid_info=""
    
    # Verificar BTRFS - Mejorado para detectar arrays multi-disco
    if command -v btrfs &> /dev/null; then
        # Primero verificar si el disco tiene un filesystem BTRFS
        if sudo btrfs filesystem show "/dev/$disk" 2>/dev/null | grep -q "uuid:"; then
            local btrfs_uuid=$(sudo btrfs filesystem show "/dev/$disk" 2>/dev/null | grep "uuid:" | awk '{print $4}')
            local btrfs_label=$(sudo btrfs filesystem show "/dev/$disk" 2>/dev/null | grep "Label:" | awk '{print $2}')
            
            # Obtener informaciÃ³n detallada del array BTRFS
            local btrfs_info=$(sudo btrfs filesystem show "/dev/$disk" 2>/dev/null)
            local total_devices=$(echo "$btrfs_info" | grep "Total devices" | awk '{print $3}')
            local device_list=$(echo "$btrfs_info" | grep "devid" | awk '{print $NF}' | sed 's|/dev/||' | tr '\n' ',' | sed 's/,$//')
            
            # Obtener el punto de montaje si existe
            local mount_point=$(mount | grep "/dev/$disk" | awk '{print $3}' | head -1)
            
            # Intentar obtener el tipo de RAID si estÃ¡ montado
            local raid_profile=""
            if [ -n "$mount_point" ]; then
                raid_profile=$(sudo btrfs filesystem usage "$mount_point" 2>/dev/null | grep "Data," | head -1 | awk '{print $1}' | cut -d',' -f2 | cut -d':' -f1)
                if [ -z "$raid_profile" ]; then
                    raid_profile="single"
                fi
            fi
            
            # Construir informaciÃ³n del RAID
            if [ "$total_devices" -gt 1 ]; then
                if [ -n "$mount_point" ]; then
                    raid_info="BTRFS RAID ($raid_profile, $total_devices discos: $device_list) - Montado en: $mount_point"
                else
                    raid_info="BTRFS RAID ($total_devices discos: $device_list)"
                fi
            else
                if [ -n "$mount_point" ]; then
                    raid_info="BTRFS (Single disk) - Montado en: $mount_point"
                else
                    raid_info="BTRFS (Single disk)"
                fi
            fi
        fi
    fi
    
    # Verificar ZFS - mÃ©todo corregido con sed
    if command -v zpool &> /dev/null; then
        # Obtener lista de todos los pools
        local all_pools=$(zpool list -H -o name 2>/dev/null)
        
        for pool in $all_pools; do
            if [ -n "$pool" ]; then
                # Obtener dispositivos del pool usando sed (funciona mejor)
                local pool_devices=$(zpool status "$pool" 2>/dev/null | sed -n '/^\s*sd[a-z]/p' | awk '{print $1}')
                
                # Verificar si el disco estÃ¡ en la lista
                if echo "$pool_devices" | grep -q "^${disk}$"; then
                    local pool_health=$(zpool list -H -o health "$pool" 2>/dev/null)
                    local pool_size=$(zpool list -H -o size "$pool" 2>/dev/null)
                    local pool_type=$(zpool status "$pool" 2>/dev/null | grep -o 'raidz[0-9]*-[0-9]*\|mirror-[0-9]*' | head -1)
                    
                    # Si no hay pool_type especÃ­fico, es un stripe
                    if [ -z "$pool_type" ]; then
                        pool_type="stripe"
                    fi
                    
                    raid_info="ZFS Pool: $pool ($pool_type, $pool_health, $pool_size)"
                    break
                fi
                
                # TambiÃ©n verificar otros tipos de dispositivos (nvme, etc.)
                if [ -z "$raid_info" ]; then
                    local all_devices=$(zpool status "$pool" 2>/dev/null | sed -n '/^\s*[a-z]/p' | grep -v 'raidz\|mirror\|spare\|log\|cache\|replacing' | awk '{print $1}')
                    if echo "$all_devices" | grep -q "^${disk}$"; then
                        local pool_health=$(zpool list -H -o health "$pool" 2>/dev/null)
                        local pool_size=$(zpool list -H -o size "$pool" 2>/dev/null)
                        local pool_type=$(zpool status "$pool" 2>/dev/null | grep -o 'raidz[0-9]*-[0-9]*\|mirror-[0-9]*' | head -1)
                        
                        # Si no hay pool_type especÃ­fico, es un stripe
                        if [ -z "$pool_type" ]; then
                            pool_type="stripe"
                        fi
                        
                        raid_info="ZFS Pool: $pool ($pool_type, $pool_health, $pool_size)"
                        break
                    fi
                fi
                
                # TambiÃ©n verificar particiones (sdc1, sdc9, etc.)
                local disk_partitions=$(lsblk -ln -o NAME "/dev/$disk" 2>/dev/null | grep -v "^${disk##*/}$")
                for partition in $disk_partitions; do
                    if echo "$pool_devices $all_devices" | grep -q "^${partition}$"; then
                        local pool_health=$(zpool list -H -o health "$pool" 2>/dev/null)
                        local pool_size=$(zpool list -H -o size "$pool" 2>/dev/null)
                        local pool_type=$(zpool status "$pool" 2>/dev/null | grep -o 'raidz[0-9]*-[0-9]*\|mirror-[0-9]*' | head -1)
                        
                        # Si no hay pool_type especÃ­fico, es un stripe
                        if [ -z "$pool_type" ]; then
                            pool_type="stripe"
                        fi
                        
                        raid_info="ZFS Pool: $pool ($pool_type, $pool_health, $pool_size)"
                        break 2
                    fi
                done
            fi
        done
    fi
    
    # Verificar mdadm (Linux Software RAID)
    if command -v mdadm &> /dev/null; then
        local md_array=$(mdadm --examine "/dev/$disk" 2>/dev/null | grep "Array UUID" | awk '{print $4}')
        if [ -n "$md_array" ]; then
            local md_name=$(mdadm --examine "/dev/$disk" 2>/dev/null | grep "Name" | awk '{print $3}')
            raid_info="mdadm RAID: $md_name (UUID: $md_array)"
        fi
    fi
    
    # Verificar LVM
    if command -v pvdisplay &> /dev/null; then
        if pvdisplay "/dev/$disk" 2>/dev/null | grep -q "PV Name"; then
            local vg_name=$(pvdisplay "/dev/$disk" 2>/dev/null | grep "VG Name" | awk '{print $3}')
            raid_info="LVM Physical Volume (VG: $vg_name)"
        fi
    fi
    
    # Verificar si tiene particiones montadas (que no sean ZFS)
    if mount | grep -q "/dev/$disk" && [ -z "$(echo "$raid_info" | grep ZFS)" ]; then
        local mount_points=$(mount | grep "/dev/$disk" | awk '{print $3}' | tr '\n' ' ')
        if [ -n "$raid_info" ]; then
            raid_info="$raid_info, Montado en: $mount_points"
        else
            raid_info="Montado en: $mount_points"
        fi
    fi
    
    echo "$raid_info"
}

# FunciÃ³n para eliminar pools ZFS especÃ­ficos de forma segura
destroy_zfs_pool_safely() {
    local pool_name="$1"
    local force="${2:-false}"
    
    if [ -z "$pool_name" ]; then
        show_error "Nombre de pool ZFS no especificado"
        return 1
    fi
    
    # Verificar que el pool existe
    if ! zpool list -H -o name 2>/dev/null | grep -q "^${pool_name}$"; then
        show_warning "Pool ZFS '$pool_name' no encontrado"
        return 0
    fi
    
    show_warning "ğŸ—‘ï¸  Preparando eliminaciÃ³n del pool ZFS: '$pool_name'"
    
    # Mostrar informaciÃ³n del pool antes de eliminar
    local pool_size=$(zpool list -H -o size "$pool_name" 2>/dev/null)
    local pool_health=$(zpool list -H -o health "$pool_name" 2>/dev/null)
    local pool_devices=$(zpool status "$pool_name" 2>/dev/null | grep -E "^\s+[a-z]" | awk '{print $1}' | grep -v "raidz\|mirror\|spare\|log\|cache\|replacing")
    
    echo "   ğŸ“Š TamaÃ±o: $pool_size"
    echo "   ğŸ’š Estado: $pool_health"
    echo "   ğŸ’¿ Dispositivos: $(echo $pool_devices | tr '\n' ' ')"
    
    # Mostrar datasets que se eliminarÃ¡n
    local datasets=$(zfs list -H -o name -r "$pool_name" 2>/dev/null)
    if [ -n "$datasets" ]; then
        echo "   ğŸ“ Datasets que se eliminarÃ¡n:"
        for dataset in $datasets; do
            local used=$(zfs list -H -o used "$dataset" 2>/dev/null)
            echo "      â€¢ $dataset (Usado: $used)"
        done
    fi
    
    # Mostrar snapshots que se eliminarÃ¡n
    local snapshots=$(zfs list -t snapshot -H -o name -r "$pool_name" 2>/dev/null)
    if [ -n "$snapshots" ]; then
        echo "   ğŸ“¸ Snapshots que se eliminarÃ¡n:"
        for snapshot in $snapshots; do
            echo "      â€¢ $snapshot"
        done
    fi
    
    echo ""
    show_warning "âš ï¸  ESTA ACCIÃ“N ELIMINARÃ PERMANENTEMENTE:"
    show_warning "    â€¢ El pool ZFS '$pool_name' completo"
    show_warning "    â€¢ Todos los datasets y sus datos"
    show_warning "    â€¢ Todos los snapshots"
    show_warning "    â€¢ ConfiguraciÃ³n de montaje automÃ¡tico"
    echo ""
    
    if [ "$force" != "true" ]; then
        if ! confirm "Â¿EstÃ¡s COMPLETAMENTE SEGURO de eliminar el pool '$pool_name'?"; then
            show_message "EliminaciÃ³n cancelada por el usuario"
            return 1
        fi
    fi
    
    # Proceder con la eliminaciÃ³n
    show_message "ğŸ”„ Eliminando pool ZFS '$pool_name'..."
    
    # 1. DesmontØ§Ø± todos los datasets
    if [ -n "$datasets" ]; then
        show_message "Desmontando datasets..."
        for dataset in $datasets; do
            sudo zfs unmount "$dataset" 2>/dev/null || true
        done
    fi
    
    # 2. Intentar exportar el pool primero
    show_message "Exportando pool..."
    sudo zpool export "$pool_name" 2>/dev/null || true
    sleep 1
    
    # 3. Destruir el pool
    show_message "Destruyendo pool..."
    if sudo zpool destroy -f "$pool_name" 2>/dev/null; then
        show_message "âœ… Pool ZFS '$pool_name' eliminado exitosamente"
    else
        show_warning "âš ï¸  Intento estÃ¡ndar fallÃ³, forzando eliminaciÃ³n..."
        
        # Intentar importar y destruir forzadamente
        sudo zpool import "$pool_name" 2>/dev/null || true
        sleep 1
        
        if sudo zpool destroy -f "$pool_name" 2>/dev/null; then
            show_message "âœ… Pool ZFS '$pool_name' eliminado forzadamente"
        else
            show_error "âŒ No se pudo eliminar el pool '$pool_name'"
            return 1
        fi
    fi
    
    # 4. Limpiar referencias en servicios de sistema
    show_message "Limpiando configuraciÃ³n del sistema..."
    
    # Limpiar cache de zpool
    sudo rm -f /etc/zfs/zpool.cache 2>/dev/null || true
    
    # Regenerar cache
    sudo zpool import -a 2>/dev/null || true
    
    return 0
}

# FunciÃ³n para eliminar filesystem BTRFS de forma segura
destroy_btrfs_safely() {
    local disk="$1"
    local force="${2:-false}"
    
    if [ -z "$disk" ]; then
        show_error "Disco no especificado para eliminaciÃ³n BTRFS"
        return 1
    fi
    
    # Verificar que es BTRFS
    if ! btrfs filesystem show "/dev/$disk" 2>/dev/null | grep -q "uuid:"; then
        show_warning "No se encontrÃ³ filesystem BTRFS en /dev/$disk"
        return 0
    fi
    
    show_warning "ğŸ—‘ï¸  Preparando eliminaciÃ³n del filesystem BTRFS en: /dev/$disk"
    
    # Obtener informaciÃ³n del filesystem
    local btrfs_uuid=$(btrfs filesystem show "/dev/$disk" 2>/dev/null | grep "uuid:" | awk '{print $4}')
    local btrfs_size=$(lsblk -dpno SIZE "/dev/$disk" | tr -d ' ')
    local mount_point=$(mount | grep "/dev/$disk" | awk '{print $3}' | head -1)
    
    echo "   ğŸ“Š TamaÃ±o: $btrfs_size"
    echo "   ğŸ†” UUID: $btrfs_uuid"
    if [ -n "$mount_point" ]; then
        echo "   ğŸ“ Montado en: $mount_point"
    fi
    
    # Mostrar subvolÃºmenes si existen
    if [ -n "$mount_point" ]; then
        local subvolumes=$(btrfs subvolume list "$mount_point" 2>/dev/null | awk '{print $9}')
        if [ -n "$subvolumes" ]; then
            echo "   ğŸ“‚ SubvolÃºmenes que se eliminarÃ¡n:"
            for subvol in $subvolumes; do
                echo "      â€¢ $subvol"
            done
        fi
    fi
    
    echo ""
    show_warning "âš ï¸  ESTA ACCIÃ“N ELIMINARÃ PERMANENTEMENTE:"
    show_warning "    â€¢ El filesystem BTRFS completo en /dev/$disk"
    show_warning "    â€¢ Todos los datos y subvolÃºmenes"
    show_warning "    â€¢ Entradas de montaje automÃ¡tico en /etc/fstab"
    echo ""
    
    if [ "$force" != "true" ]; then
        if ! confirm "Â¿EstÃ¡s COMPLETAMENTE SEGURO de eliminar el BTRFS en /dev/$disk?"; then
            show_message "EliminaciÃ³n cancelada por el usuario"
            return 1
        fi
    fi
    
    # Proceder con la eliminaciÃ³n
    show_message "ğŸ”„ Eliminando filesystem BTRFS en /dev/$disk..."
    
    # 1. Desmontar el filesystem
    if [ -n "$mount_point" ]; then
        show_message "Desmontando $mount_point..."
        sudo umount "$mount_point" 2>/dev/null || sudo umount -f "$mount_point" 2>/dev/null || true
    fi
    
    # Desmontar cualquier particiÃ³n del disco
    sudo umount "/dev/$disk"* 2>/dev/null || true
    
    # 2. Eliminar entradas de /etc/fstab especÃ­ficas de BTRFS de forma segura
    show_message "Eliminando entradas BTRFS de /etc/fstab..."
    if [ -n "$btrfs_uuid" ]; then
        # Crear backup de fstab con timestamp
        local backup_file="/etc/fstab.backup.$(date +%Y%m%d-%H%M%S)"
        sudo cp /etc/fstab "$backup_file" 2>/dev/null || true
        show_message "ğŸ“‹ Backup de fstab creado: $backup_file"
        
        # Verificar quÃ© entradas existen antes de eliminar
        local existing_entries=$(grep -E "UUID=$btrfs_uuid" /etc/fstab 2>/dev/null | grep -i btrfs || true)
        if [ -n "$existing_entries" ]; then
            show_message "ğŸ“ Entradas BTRFS encontradas en /etc/fstab:"
            echo "$existing_entries" | while read -r entry; do
                echo "   â€¢ $entry"
            done
            
            # Eliminar entradas por UUID solo si el tipo de filesystem es btrfs
            # Usar patrones mÃ¡s especÃ­ficos para mayor seguridad
            sudo sed -i "/^[[:space:]]*UUID=$btrfs_uuid[[:space:]].*[[:space:]]btrfs[[:space:]]/d" /etc/fstab 2>/dev/null || true
            sudo sed -i "/^[[:space:]]*UUID=$btrfs_uuid[[:space:]].*[[:space:]]btrfs$/d" /etc/fstab 2>/dev/null || true
            
            # Verificar si las entradas fueron eliminadas
            local remaining_entries=$(grep -E "UUID=$btrfs_uuid" /etc/fstab 2>/dev/null | grep -i btrfs || true)
            if [ -z "$remaining_entries" ]; then
                show_message "âœ… Entradas BTRFS eliminadas exitosamente de /etc/fstab"
            else
                show_warning "âš ï¸  Algunas entradas BTRFS permanecen en /etc/fstab:"
                echo "$remaining_entries" | while read -r entry; do
                    echo "   â€¢ $entry"
                done
            fi
        else
            show_message "â„¹ï¸  No se encontraron entradas BTRFS automount en /etc/fstab para UUID: $btrfs_uuid"
        fi
    fi
    
    # TambiÃ©n verificar entradas por device path (menos comÃºn pero posible)
    local device_entries=$(grep -E "/dev/$disk" /etc/fstab 2>/dev/null | grep -i btrfs || true)
    if [ -n "$device_entries" ]; then
        show_message "ğŸ“ Entradas por device path encontradas:"
        echo "$device_entries" | while read -r entry; do
            echo "   â€¢ $entry"
        done
        
        # Eliminar por device path pero solo lÃ­neas que contengan btrfs
        sudo sed -i "\|^[[:space:]]*/dev/$disk[[:space:]].*[[:space:]]btrfs[[:space:]]|d" /etc/fstab 2>/dev/null || true
        sudo sed -i "\|^[[:space:]]*/dev/$disk[0-9]*[[:space:]].*[[:space:]]btrfs[[:space:]]|d" /etc/fstab 2>/dev/null || true
        
        # Verificar si fueron eliminadas
        local remaining_device_entries=$(grep -E "/dev/$disk" /etc/fstab 2>/dev/null | grep -i btrfs || true)
        if [ -z "$remaining_device_entries" ]; then
            show_message "âœ… Entradas por device path eliminadas exitosamente"
        fi
    fi
    
    # 3. Limpiar metadatos BTRFS
    show_message "Limpiando metadatos BTRFS..."
    sudo wipefs -af "/dev/$disk" 2>/dev/null || true
    
    show_message "âœ… Filesystem BTRFS en /dev/$disk eliminado exitosamente"
    return 0
}

# FunciÃ³n para eliminar array BTRFS completo de forma segura
destroy_btrfs_array_safely() {
    local primary_disk="$1"
    local array_uuid="$2"
    
    if [ -z "$primary_disk" ] || [ -z "$array_uuid" ]; then
        show_error "ParÃ¡metros incorrectos para eliminaciÃ³n de array BTRFS"
        return 1
    fi
    
    show_warning "ğŸ—‘ï¸  Eliminando array BTRFS completo (UUID: $array_uuid)"
    
    # Obtener informaciÃ³n completa del array
    local btrfs_info=$(sudo btrfs filesystem show "/dev/$primary_disk" 2>/dev/null)
    local total_devices=$(echo "$btrfs_info" | grep "Total devices" | awk '{print $3}')
    local all_devices=$(echo "$btrfs_info" | grep "devid" | awk '{print $NF}' | sed 's|/dev/||')
    local mount_point=$(mount | grep "UUID=$array_uuid" | awk '{print $3}' | head -1)
    
    echo "   ğŸ“Š Total de dispositivos: $total_devices"
    echo "   ğŸ“€ Dispositivos: $all_devices"
    if [ -n "$mount_point" ]; then
        echo "   ğŸ“ Montado en: $mount_point"
    fi
    
    # Obtener tipo de RAID si estÃ¡ montado
    local raid_profile=""
    if [ -n "$mount_point" ]; then
        raid_profile=$(sudo btrfs filesystem usage "$mount_point" 2>/dev/null | grep "Data," | head -1 | awk '{print $1}' | cut -d',' -f2 | cut -d':' -f1)
        if [ -n "$raid_profile" ]; then
            echo "   ğŸ”§ ConfiguraciÃ³n RAID: $raid_profile"
        fi
    fi
    
    echo ""
    show_warning "âš ï¸  ESTA ACCIÃ“N ELIMINARÃ PERMANENTEMENTE:"
    show_warning "    â€¢ Todo el array BTRFS ($total_devices dispositivos)"
    show_warning "    â€¢ Todos los datos y subvolÃºmenes"
    show_warning "    â€¢ Entradas de montaje automÃ¡tico en /etc/fstab"
    echo ""
    
    if ! confirm "Â¿EstÃ¡s COMPLETAMENTE SEGURO de eliminar este array BTRFS completo?"; then
        show_message "EliminaciÃ³n cancelada por el usuario"
        return 1
    fi
    
    # Proceder con la eliminaciÃ³n del array completo
    show_message "ğŸ”„ Eliminando array BTRFS..."
    
    # 1. Desmontar el filesystem
    if [ -n "$mount_point" ]; then
        show_message "Desmontando $mount_point..."
        sudo umount "$mount_point" 2>/dev/null || sudo umount -f "$mount_point" 2>/dev/null || true
    fi
    
    # Desmontar todos los dispositivos del array
    for device in $all_devices; do
        sudo umount "/dev/$device"* 2>/dev/null || true
    done
    
    # 2. Eliminar entradas de /etc/fstab para el array completo
    show_message "Eliminando entradas del array BTRFS de /etc/fstab..."
    local backup_file="/etc/fstab.backup.$(date +%Y%m%d-%H%M%S)"
    sudo cp /etc/fstab "$backup_file" 2>/dev/null || true
    show_message "ğŸ“‹ Backup de fstab creado: $backup_file"
    
    # Verificar entradas existentes
    local existing_entries=$(grep -E "UUID=$array_uuid" /etc/fstab 2>/dev/null | grep -i btrfs || true)
    if [ -n "$existing_entries" ]; then
        show_message "ğŸ“ Eliminando entradas del array BTRFS:"
        echo "$existing_entries" | while read -r entry; do
            echo "   â€¢ $entry"
        done
        
        # Eliminar entradas por UUID del array
        sudo sed -i "/^[[:space:]]*UUID=$array_uuid[[:space:]].*[[:space:]]btrfs[[:space:]]/d" /etc/fstab 2>/dev/null || true
        sudo sed -i "/^[[:space:]]*UUID=$array_uuid[[:space:]].*[[:space:]]btrfs$/d" /etc/fstab 2>/dev/null || true
        
        # Verificar eliminaciÃ³n
        local remaining_entries=$(grep -E "UUID=$array_uuid" /etc/fstab 2>/dev/null | grep -i btrfs || true)
        if [ -z "$remaining_entries" ]; then
            show_message "âœ… Entradas del array eliminadas exitosamente de /etc/fstab"
        else
            show_warning "âš ï¸  Algunas entradas del array permanecen en /etc/fstab"
        fi
    else
        show_message "â„¹ï¸  No se encontraron entradas automount para este array"
    fi
    
    # 3. Limpiar metadatos BTRFS de todos los dispositivos
    show_message "Limpiando metadatos BTRFS de todos los dispositivos..."
    for device in $all_devices; do
        show_message "  Limpiando /dev/$device..."
        sudo wipefs -af "/dev/$device" 2>/dev/null || true
    done
    
    show_message "âœ… Array BTRFS completo eliminado exitosamente"
    show_message "   ğŸ“€ Dispositivos limpiados: $all_devices"
    return 0
}

# FunciÃ³n para limpiar disco de configuraciones RAID anteriores
clean_disk() {
    local disk="$1"
    
    show_message "Limpiando disco /dev/$disk..."
    
    # Desmontar si estÃ¡ montado
    if mount | grep -q "/dev/$disk"; then
        show_message "Desmontando /dev/$disk..."
        sudo umount "/dev/$disk"* 2>/dev/null || true
    fi
    
    # Verificar y destruir pools ZFS que usen este disco
    if command -v zpool &> /dev/null; then
        show_message "Verificando pools ZFS que usan /dev/$disk..."
        
        # Obtener lista de todos los pools
        local all_pools=$(zpool list -H -o name 2>/dev/null)
        local pools_to_destroy=()
        
        for pool in $all_pools; do
            if [ -n "$pool" ]; then
                # Verificar si este disco estÃ¡ en el pool
                if zpool status "$pool" 2>/dev/null | sed -n '/^\s*sd[a-z]/p' | awk '{print $1}' | grep -q "^${disk}$"; then
                    pools_to_destroy+=("$pool")
                elif zpool status "$pool" 2>/dev/null | sed -n '/^\s*[a-z]/p' | grep -v 'raidz\|mirror\|spare\|log\|cache\|replacing' | awk '{print $1}' | grep -q "^${disk}$"; then
                    pools_to_destroy+=("$pool")
                fi
                
                # TambiÃ©n verificar particiones
                local disk_partitions=$(lsblk -ln -o NAME "/dev/$disk" 2>/dev/null | grep -v "^${disk##*/}$")
                for partition in $disk_partitions; do
                    if zpool status "$pool" 2>/dev/null | grep -q "$partition"; then
                        pools_to_destroy+=("$pool")
                        break
                    fi
                done
            fi
        done
        
        # Eliminar duplicados de pools
        local unique_pools=($(printf "%s\n" "${pools_to_destroy[@]}" | sort -u))
        
        # Destruir pools encontrados usando la funciÃ³n segura
        for pool in "${unique_pools[@]}"; do
            if [ -n "$pool" ]; then
                show_warning "ğŸ—‘ï¸  Detectado pool ZFS '$pool' usando el disco /dev/$disk"
                
                # Usar la funciÃ³n de eliminaciÃ³n segura
                if ! destroy_zfs_pool_safely "$pool" "true"; then
                    show_error "âŒ Error eliminando el pool '$pool'"
                    show_message "Continuando con limpieza manual..."
                    
                    # Fallback a mÃ©todo anterior si la funciÃ³n segura falla
                    show_warning "âš ï¸  Intentando mÃ©todo de emergencia..."
                    sudo zpool export "$pool" 2>/dev/null || true
                    sleep 1
                    sudo zpool destroy -f "$pool" 2>/dev/null || true
                fi
            fi
        done
        
        # Limpiar etiquetas ZFS residuales en el disco y particiones
        sudo zpool labelclear -f "/dev/$disk" 2>/dev/null || true
        
        # Limpiar particiones ZFS si existen
        local disk_partitions=$(lsblk -ln -o NAME "/dev/$disk" 2>/dev/null | grep -v "^${disk##*/}$")
        for partition in $disk_partitions; do
            local full_partition="/dev/${partition}"
            sudo zpool labelclear -f "$full_partition" 2>/dev/null || true
        done
    fi
    
    # Verificar y limpiar BTRFS usando la funciÃ³n segura
    if command -v btrfs &> /dev/null; then
        if btrfs filesystem show "/dev/$disk" 2>/dev/null | grep -q "uuid:"; then
            show_warning "ğŸ—‘ï¸  Detectado filesystem BTRFS en /dev/$disk"
            
            # Usar la funciÃ³n de eliminaciÃ³n segura para BTRFS
            if ! destroy_btrfs_safely "$disk" "true"; then
                show_error "âŒ Error eliminando filesystem BTRFS"
                show_message "Continuando con limpieza manual..."
                
                # Fallback a limpieza bÃ¡sica
                sudo wipefs -a "/dev/$disk" 2>/dev/null || true
            fi
        fi
    fi
    
    # Limpiar metadatos mdadm
    if command -v mdadm &> /dev/null; then
        sudo mdadm --zero-superblock "/dev/$disk" 2>/dev/null || true
    fi
    
    # Limpiar metadatos LVM
    if command -v pvremove &> /dev/null; then
        sudo pvremove -ff "/dev/$disk" 2>/dev/null || true
    fi
    
    # Limpiar tabla de particiones y metadatos completamente
    show_message "ğŸ§¹ Limpiando tabla de particiones y metadatos de /dev/$disk..."
    
    # Limpiar los primeros 100MB y los Ãºltimos 100MB del disco
    sudo dd if=/dev/zero of="/dev/$disk" bs=1M count=100 2>/dev/null || true
    
    # Obtener el tamaÃ±o del disco y limpiar el final tambiÃ©n
    local disk_size=$(lsblk -dpno SIZE "/dev/$disk" --bytes 2>/dev/null || echo "0")
    if [ "$disk_size" -gt 104857600 ]; then  # Si es mayor a 100MB
        local seek_pos=$(( (disk_size / 1048576) - 100 ))  # 100MB antes del final
        sudo dd if=/dev/zero of="/dev/$disk" bs=1M seek="$seek_pos" count=100 2>/dev/null || true
    fi
    
    # Limpiar con wipefs para asegurar que no queden metadatos
    sudo wipefs -af "/dev/$disk" 2>/dev/null || true
    
    # Forzar actualizaciÃ³n de la tabla de particiones
    sudo partprobe "/dev/$disk" 2>/dev/null || true
    sudo udevadm settle 2>/dev/null || true
    
    # Esperar un poco mÃ¡s para que el sistema reconozca los cambios
    sleep 3
    
    show_message "âœ… Disco /dev/$disk limpiado completamente"
}

# FunciÃ³n para detectar discos disponibles
detect_disks() {
    show_title "Detectando discos disponibles"
    
    # Obtener lista de discos, excluyendo dispositivos del sistema
    AVAILABLE_DISKS=()
    DISK_RAID_STATUS=()
    
    # Obtener disco raÃ­z
    ROOT_DISK=$(lsblk -no PKNAME $(findmnt -n -o SOURCE /))
    
    # Obtener discos con particiones montadas en puntos crÃ­ticos del sistema
    SYSTEM_DISKS=()
    SYSTEM_DISKS+=("$ROOT_DISK")
    
    # Detectar discos que contienen particiones del sistema
    while IFS= read -r device mount_point; do        
        # Excluir puntos de montaje crÃ­ticos del sistema
        if [[ "$mount_point" == "/" ]] || [[ "$mount_point" == "/boot"* ]] || \
           [[ "$mount_point" == "/usr"* ]] || [[ "$mount_point" == "/var"* ]] || \
           [[ "$mount_point" == "/etc"* ]] || [[ "$mount_point" == "/lib"* ]] || \
           [[ "$mount_point" == "/bin"* ]] || [[ "$mount_point" == "/sbin"* ]]; then
            
            # Extraer el disco base del device
            local base_disk=$(lsblk -no PKNAME "$device" 2>/dev/null || echo "$device" | sed 's|/dev/||' | sed 's/[0-9]*$//')
            if [ -n "$base_disk" ] && [[ ! " ${SYSTEM_DISKS[*]} " =~ " ${base_disk} " ]]; then
                SYSTEM_DISKS+=("$base_disk")
            fi
        fi
    done < <(findmnt -rn -o SOURCE,TARGET | grep -E "^/dev/")
    
    show_message "ğŸ“‹ Discos del sistema excluidos: ${SYSTEM_DISKS[*]}"
    
    while IFS= read -r disk; do
        # Excluir discos del sistema, dispositivos loop, y particiones
        if [[ ! " ${SYSTEM_DISKS[*]} " =~ " ${disk} " ]] && \
           [[ "$disk" != loop* ]] && [[ "$disk" != ram* ]] && \
           [[ "$disk" != *boot* ]] && [[ ! "$disk" =~ [0-9]$ ]]; then
            
            # Verificar que sea un disco completo, no una particiÃ³n
            if lsblk -dpno TYPE "/dev/$disk" 2>/dev/null | grep -q "disk"; then
                AVAILABLE_DISKS+=("$disk")
                # Verificar estado RAID del disco
                raid_status=$(check_disk_raid_status "$disk")
                DISK_RAID_STATUS+=("$raid_status")
            fi
        fi
    done < <(lsblk -dpno NAME | sed 's|/dev/||' | grep -v '^$')
    
    if [ ${#AVAILABLE_DISKS[@]} -eq 0 ]; then
        show_error "No se encontraron discos disponibles para RAID"
        exit 1
    fi
    
    show_message "Discos encontrados:"
    local disks_with_raid=false
    
    for i in "${!AVAILABLE_DISKS[@]}"; do
        disk="/dev/${AVAILABLE_DISKS[$i]}"
        size=$(lsblk -dpno SIZE "$disk" | tr -d ' ')
        model=$(lsblk -dpno MODEL "$disk" | tr -d ' ')
        status="${DISK_RAID_STATUS[$i]}"
        
        if [ -n "$status" ]; then
            echo -e "  $((i+1)). ${AVAILABLE_DISKS[$i]} - $size - $model ${RED}[EN USO: $status]${NC}"
            disks_with_raid=true
        else
            echo "  $((i+1)). ${AVAILABLE_DISKS[$i]} - $size - $model [LIBRE]"
        fi
    done
    
    # Si hay discos en RAID existentes, preguntar al usuario
    if [ "$disks_with_raid" = true ]; then
        show_warning "Â¡ATENCIÃ“N! Algunos discos estÃ¡n siendo utilizados en configuraciones RAID existentes."
        show_warning "Si continÃºas, tendrÃ¡s la opciÃ³n de limpiar estos discos antes de crear el nuevo RAID."
        show_warning "Esto DESTRUIRÃ todos los datos en esos discos."
        
        if ! confirm "Â¿Deseas continuar con la configuraciÃ³n?"; then
            show_message "ConfiguraciÃ³n cancelada por el usuario"
            exit 0
        fi
    fi
    
    # Detectar NVME
    NVME_DISK=""
    for disk in "${AVAILABLE_DISKS[@]}"; do
        if [[ "$disk" == nvme* ]]; then
            NVME_DISK="$disk"
            show_message "Detectado disco NVME: $NVME_DISK"
            break
        fi
    done
}

# FunciÃ³n para seleccionar tipo de filesystem
select_filesystem() {
    show_title "SelecciÃ³n de Sistema de Archivos"
    echo "Selecciona el tipo de sistema de archivos RAID:"
    echo "1. BTRFS"
    echo "2. ZFS"
    
    while true; do
        read -p "Selecciona una opciÃ³n (1-2): " choice
        case $choice in
            1)
                FILESYSTEM_TYPE="btrfs"
                show_warning "NOTA: En BTRFS, RAID 5/6 aÃºn es experimental"
                break
                ;;
            2)
                FILESYSTEM_TYPE="zfs"
                break
                ;;
            *)
                echo "OpciÃ³n invÃ¡lida. Por favor selecciona 1 o 2."
                ;;
        esac
    done
}

# FunciÃ³n para mostrar tipos de RAID
show_raid_types() {
    if [ "$FILESYSTEM_TYPE" = "btrfs" ]; then
        echo "Tipos de RAID disponibles en BTRFS:"
        echo "1. RAID 0 (stripe) - No redundancy, maximum performance and capacity"
        echo "2. RAID 1 (mirror) - Data mirrored across drives, 50% capacity"
        echo "3. RAID 5 - Single drive fault tolerance with parity (EXPERIMENTAL)"
        echo "4. RAID 6 - Dual drive fault tolerance with parity (EXPERIMENTAL)"
        echo "5. RAID 10 - Combination of RAID 0 and 1, requires 4+ drives"
    else
        echo "Tipos de RAID disponibles en ZFS:"
        echo "1. Stripe - No redundancy, maximum performance"
        echo "2. Mirror - Data mirrored across drives"
        echo "3. RAIDZ1 - Single parity, equivalent to RAID 5"
        echo "4. RAIDZ2 - Double parity, equivalent to RAID 6"
        echo "5. RAIDZ3 - Triple parity, requires 4+ drives"
    fi
}

# FunciÃ³n para calcular capacidad del RAID
calculate_raid_capacity() {
    local raid_type="$1"
    local disk_sizes=("${@:2}")
    local total_capacity=0
    local min_size=${disk_sizes[0]}
    local num_disks=${#disk_sizes[@]}
    
    # Encontrar el disco mÃ¡s pequeÃ±o
    for size in "${disk_sizes[@]}"; do
        if [ "$size" -lt "$min_size" ]; then
            min_size=$size
        fi
        total_capacity=$((total_capacity + size))
    done
    
    # Calcular capacidad segÃºn tipo de RAID
    case $raid_type in
        "raid0"|"stripe")
            RAID_CAPACITY=$total_capacity
            ;;
        "raid1"|"mirror")
            RAID_CAPACITY=$min_size
            ;;
        "raid5"|"raidz1")
            RAID_CAPACITY=$(( min_size * (num_disks - 1) ))
            ;;
        "raid6"|"raidz2")
            RAID_CAPACITY=$(( min_size * (num_disks - 2) ))
            ;;
        "raid10")
            RAID_CAPACITY=$(( min_size * (num_disks / 2) ))
            ;;
        "raidz3")
            RAID_CAPACITY=$(( min_size * (num_disks - 3) ))
            ;;
    esac
    
    # Mostrar informaciÃ³n si hay discos de diferente tamaÃ±o
    local sizes_differ=false
    for size in "${disk_sizes[@]}"; do
        if [ "$size" -ne "${disk_sizes[0]}" ]; then
            sizes_differ=true
            break
        fi
    done
    
    if [ "$sizes_differ" = true ]; then
        show_warning "Â¡ATENCIÃ“N! Los discos tienen diferentes tamaÃ±os:"
        for i in "${!SELECTED_DISKS[@]}"; do
            echo "  ${SELECTED_DISKS[$i]}: $(( ${disk_sizes[$i]} / 1024 / 1024 / 1024 )) GB"
        done
        echo "Capacidad del RAID resultante: $(( RAID_CAPACITY / 1024 / 1024 / 1024 )) GB"
        echo "Se utilizarÃ¡ el tamaÃ±o del disco mÃ¡s pequeÃ±o como referencia."
        
        if ! confirm "Â¿Deseas continuar con esta configuraciÃ³n?"; then
            show_message "ConfiguraciÃ³n cancelada por el usuario"
            exit 0
        fi
    fi
}

# FunciÃ³n para seleccionar discos
select_disks() {
    show_title "SelecciÃ³n de Discos"
    
    local min_disks=2
    case $RAID_TYPE in
        "raid5"|"raidz1") min_disks=3;;
        "raid6"|"raidz2") min_disks=4;;
        "raid10") min_disks=4;;
        "raidz3") min_disks=4;;
    esac
    
    show_message "Selecciona los discos para el RAID (mÃ­nimo $min_disks discos):"
    echo ""
    echo "ğŸ“– INSTRUCCIONES DE SELECCIÃ“N:"
    echo "   SelecciÃ³n individual: Escribe el nÃºmero del disco (ej: 3)"
    echo "   SelecciÃ³n mÃºltiple:   Escribe nÃºmeros separados por espacios (ej: 3 4 5 6)"
    echo "   SelecciÃ³n por rango:  Escribe rango con guiÃ³n (ej: 3-6)"
    echo "   Quitar selecciÃ³n:     Vuelve a escribir el nÃºmero del disco"
    echo ""
    echo "ğŸ¯ COMANDOS ESPECIALES:"
    echo "   'all'   - Seleccionar todos los discos libres"
    echo "   'zfs'   - Seleccionar todos los discos ZFS (requiere confirmaciÃ³n)"
    echo "   'clear' - Limpiar toda la selecciÃ³n"
    echo "   'done'  - Finalizar selecciÃ³n y continuar"
    echo ""
    
    SELECTED_DISKS=()
    SELECTED_DISK_SIZES=()
    DISKS_TO_CLEAN=()
    
    while true; do
        echo "Discos disponibles:"
        for i in "${!AVAILABLE_DISKS[@]}"; do
            disk="/dev/${AVAILABLE_DISKS[$i]}"
            size=$(lsblk -dpno SIZE "$disk" | tr -d ' ')
            status="${DISK_RAID_STATUS[$i]}"
            
            if [ -n "$status" ]; then
                echo -e "  $((i+1)). ${AVAILABLE_DISKS[$i]} - $size ${RED}[EN USO: $status]${NC}"
            else
                echo "  $((i+1)). ${AVAILABLE_DISKS[$i]} - $size [LIBRE]"
            fi
        done
        
        echo ""
        if [ ${#SELECTED_DISKS[@]} -gt 0 ]; then
            echo "âœ… Discos seleccionados (${#SELECTED_DISKS[@]}/${#AVAILABLE_DISKS[@]}): ${SELECTED_DISKS[*]}"
        else
            echo "â­• Discos seleccionados: ninguno"
        fi
        echo ""
        echo "ğŸ’¡ EJEMPLOS DE USO:"
        if [ ${#SELECTED_DISKS[@]} -eq 0 ]; then
            if [ "$FILESYSTEM_TYPE" = "zfs" ]; then
                echo "   Para RAID ZFS:  escribe 'zfs' o '3 4 5 6'"
            else
                echo "   Para RAID BTRFS: escribe 'btrfs' o '3 4 5 6'"
            fi
            echo "   Para algunos:   escribe '3 4' o '3-4'"
            echo "   Individual:     escribe '3' luego '4', etc."
        else
            echo "   Agregar mÃ¡s:    escribe '${#SELECTED_DISKS[@]}+1' o nÃºmeros adicionales"
            echo "   Quitar uno:     escribe el nÃºmero de un disco ya seleccionado"
            echo "   Terminar:       escribe 'done' (necesitas ${min_disks} mÃ­nimo)"
        fi
        echo ""
        
        read -p "ğŸ‘‰ SelecciÃ³n: " choice
        
        if [ "$choice" = "done" ]; then
            if [ ${#SELECTED_DISKS[@]} -ge $min_disks ]; then
                break
            else
                show_error "âŒ Necesitas al menos $min_disks discos para este RAID"
            fi
        elif [ "$choice" = "clear" ]; then
            SELECTED_DISKS=()
            SELECTED_DISK_SIZES=()
            DISKS_TO_CLEAN=()
            show_message "ğŸ§¹ SelecciÃ³n limpiada"
        elif [ "$choice" = "all" ]; then
            # Seleccionar todos los discos libres
            for i in "${!AVAILABLE_DISKS[@]}"; do
                if [ -z "${DISK_RAID_STATUS[$i]}" ]; then
                    disk="${AVAILABLE_DISKS[$i]}"
                    if [[ ! " ${SELECTED_DISKS[*]} " =~ " ${disk} " ]]; then
                        SELECTED_DISKS+=("$disk")
                        disk_size=$(lsblk -dpno SIZE "/dev/$disk" --bytes)
                        SELECTED_DISK_SIZES+=("$disk_size")
                    fi
                fi
            done
            show_message "âœ… Se seleccionaron todos los discos libres"
        elif [ "$choice" = "zfs" ]; then
            # Seleccionar todos los discos ZFS con confirmaciÃ³n
            local zfs_disks=()
            for i in "${!AVAILABLE_DISKS[@]}"; do
                if [[ "${DISK_RAID_STATUS[$i]}" == *"ZFS"* ]]; then
                    zfs_disks+=("${AVAILABLE_DISKS[$i]}")
                fi
            done
            
            if [ ${#zfs_disks[@]} -gt 0 ]; then
                show_warning "âš ï¸  Esto seleccionarÃ¡ todos los discos ZFS: ${zfs_disks[*]}"
                show_warning "âš ï¸  Estos discos serÃ¡n COMPLETAMENTE BORRADOS"
                if confirm "Â¿Deseas seleccionar todos los discos ZFS?"; then
                    for disk in "${zfs_disks[@]}"; do
                        if [[ ! " ${SELECTED_DISKS[*]} " =~ " ${disk} " ]]; then
                            SELECTED_DISKS+=("$disk")
                            disk_size=$(lsblk -dpno SIZE "/dev/$disk" --bytes)
                            SELECTED_DISK_SIZES+=("$disk_size")
                            DISKS_TO_CLEAN+=("$disk")
                        fi
                    done
                    show_message "âœ… Se seleccionaron todos los discos ZFS"
                fi
            else
                show_warning "âŒ No hay discos ZFS disponibles"
            fi
        elif [ "$choice" = "btrfs" ]; then
            # Seleccionar todos los discos BTRFS con confirmaciÃ³n
            local btrfs_disks=()
            for i in "${!AVAILABLE_DISKS[@]}"; do
                if [[ "${DISK_RAID_STATUS[$i]}" == *"BTRFS"* ]]; then
                    btrfs_disks+=("${AVAILABLE_DISKS[$i]}")
                fi
            done
            
            if [ ${#btrfs_disks[@]} -gt 0 ]; then
                show_warning "âš ï¸  Esto seleccionarÃ¡ todos los discos BTRFS: ${btrfs_disks[*]}"
                show_warning "âš ï¸  Estos discos serÃ¡n COMPLETAMENTE BORRADOS"
                if confirm "Â¿Deseas seleccionar todos los discos BTRFS?"; then
                    for disk in "${btrfs_disks[@]}"; do
                        if [[ ! " ${SELECTED_DISKS[*]} " =~ " ${disk} " ]]; then
                            SELECTED_DISKS+=("$disk")
                            disk_size=$(lsblk -dpno SIZE "/dev/$disk" --bytes)
                            SELECTED_DISK_SIZES+=("$disk_size")
                            DISKS_TO_CLEAN+=("$disk")
                        fi
                    done
                    show_message "âœ… Se seleccionaron todos los discos BTRFS"
                fi
            else
                show_warning "âŒ No hay discos BTRFS disponibles"
            fi
        elif [[ "$choice" =~ ^[0-9]+(-[0-9]+)?$ ]]; then
            # Manejar rangos (ej: 3-6)
            if [[ "$choice" =~ - ]]; then
                local start=$(echo "$choice" | cut -d'-' -f1)
                local end=$(echo "$choice" | cut -d'-' -f2)
                
                if [ "$start" -ge 1 ] && [ "$end" -le ${#AVAILABLE_DISKS[@]} ] && [ "$start" -le "$end" ]; then
                    for ((i=start; i<=end; i++)); do
                        local disk_index=$((i-1))
                        local disk="${AVAILABLE_DISKS[$disk_index]}"
                        local disk_status="${DISK_RAID_STATUS[$disk_index]}"
                        
                        # Procesar disco (similar a la lÃ³gica individual)
                        if [[ ! " ${SELECTED_DISKS[*]} " =~ " ${disk} " ]]; then
                            if [ -n "$disk_status" ]; then
                                show_warning "âš ï¸  Disco $disk estÃ¡ en uso: $disk_status"
                                if confirm "Â¿Limpiar $disk y agregarlo?"; then
                                    SELECTED_DISKS+=("$disk")
                                    disk_size=$(lsblk -dpno SIZE "/dev/$disk" --bytes)
                                    SELECTED_DISK_SIZES+=("$disk_size")
                                    DISKS_TO_CLEAN+=("$disk")
                                fi
                            else
                                SELECTED_DISKS+=("$disk")
                                disk_size=$(lsblk -dpno SIZE "/dev/$disk" --bytes)
                                SELECTED_DISK_SIZES+=("$disk_size")
                            fi
                        fi
                    done
                    show_message "âœ… Procesado rango $start-$end"
                else
                    echo "âŒ Rango invÃ¡lido. Usa formato: inicio-fin (ej: 3-6)"
                fi
            else
                # SelecciÃ³n individual (lÃ³gica original)
                if [ "$choice" -ge 1 ] && [ "$choice" -le ${#AVAILABLE_DISKS[@]} ]; then
                    disk_index=$((choice-1))
                    disk="${AVAILABLE_DISKS[$disk_index]}"
                    disk_status="${DISK_RAID_STATUS[$disk_index]}"
                    
                    # Verificar si ya estÃ¡ seleccionado
                    if [[ " ${SELECTED_DISKS[*]} " =~ " ${disk} " ]]; then
                        # Quitar de la selecciÃ³n
                        new_selected=()
                        new_sizes=()
                        new_to_clean=()
                        for i in "${!SELECTED_DISKS[@]}"; do
                            if [ "${SELECTED_DISKS[$i]}" != "$disk" ]; then
                                new_selected+=("${SELECTED_DISKS[$i]}")
                                new_sizes+=("${SELECTED_DISK_SIZES[$i]}")
                            fi
                        done
                        SELECTED_DISKS=("${new_selected[@]}")
                        SELECTED_DISK_SIZES=("${new_sizes[@]}")
                        # Remover de discos a limpiar si estaba ahÃ­
                        for i in "${!DISKS_TO_CLEAN[@]}"; do
                            if [ "${DISKS_TO_CLEAN[$i]}" != "$disk" ]; then
                                new_to_clean+=("${DISKS_TO_CLEAN[$i]}")
                            fi
                        done
                        DISKS_TO_CLEAN=("${new_to_clean[@]}")
                        show_message "â– Removido: $disk"
                    else
                        # Verificar si el disco estÃ¡ en uso
                        if [ -n "$disk_status" ]; then
                            show_warning "âš ï¸  El disco $disk estÃ¡ en uso: $disk_status"
                            show_warning "âš ï¸  Si continÃºas, este disco serÃ¡ completamente limpiado y se perderÃ¡n TODOS los datos."
                            
                            if confirm "Â¿Deseas limpiar este disco y usarlo para el nuevo RAID?"; then
                                SELECTED_DISKS+=("$disk")
                                disk_size=$(lsblk -dpno SIZE "/dev/$disk" --bytes)
                                SELECTED_DISK_SIZES+=("$disk_size")
                                DISKS_TO_CLEAN+=("$disk")
                                show_message "âœ… Agregado (se limpiarÃ¡): $disk"
                            else
                                show_message "âŒ Disco $disk no seleccionado"
                            fi
                        else
                            # Agregar a la selecciÃ³n (disco libre)
                            SELECTED_DISKS+=("$disk")
                            disk_size=$(lsblk -dpno SIZE "/dev/$disk" --bytes)
                            SELECTED_DISK_SIZES+=("$disk_size")
                            show_message "âœ… Agregado: $disk"
                        fi
                    fi
                else
                    echo "âŒ NÃºmero invÃ¡lido. Usa 1-${#AVAILABLE_DISKS[@]}"
                fi
            fi
        elif [[ "$choice" =~ ^[0-9\ ]+$ ]]; then
            # Manejar mÃºltiples nÃºmeros separados por espacios (ej: 3 4 5 6)
            for num in $choice; do
                if [ "$num" -ge 1 ] && [ "$num" -le ${#AVAILABLE_DISKS[@]} ]; then
                    disk_index=$((num-1))
                    disk="${AVAILABLE_DISKS[$disk_index]}"
                    disk_status="${DISK_RAID_STATUS[$disk_index]}"
                    
                    if [[ ! " ${SELECTED_DISKS[*]} " =~ " ${disk} " ]]; then
                        if [ -n "$disk_status" ]; then
                            show_warning "âš ï¸  Disco $disk estÃ¡ en uso: $disk_status"
                            if confirm "Â¿Limpiar $disk y agregarlo?"; then
                                SELECTED_DISKS+=("$disk")
                                disk_size=$(lsblk -dpno SIZE "/dev/$disk" --bytes)
                                SELECTED_DISK_SIZES+=("$disk_size")
                                DISKS_TO_CLEAN+=("$disk")
                                show_message "âœ… Agregado (se limpiarÃ¡): $disk"
                            fi
                        else
                            SELECTED_DISKS+=("$disk")
                            disk_size=$(lsblk -dpno SIZE "/dev/$disk" --bytes)
                            SELECTED_DISK_SIZES+=("$disk_size")
                            show_message "âœ… Agregado: $disk"
                        fi
                    else
                        show_message "â„¹ï¸  $disk ya estaba seleccionado"
                    fi
                else
                    echo "âŒ NÃºmero invÃ¡lido: $num (usa 1-${#AVAILABLE_DISKS[@]})"
                fi
            done
        else
            echo "OpciÃ³n invÃ¡lida"
        fi
    done
    
    # Si hay discos para limpiar, mostrar resumen y confirmar
    if [ ${#DISKS_TO_CLEAN[@]} -gt 0 ]; then
        show_warning "RESUMEN DE DISCOS A LIMPIAR:"
        for disk in "${DISKS_TO_CLEAN[@]}"; do
            # Encontrar el Ã­ndice del disco para mostrar su estado
            for i in "${!AVAILABLE_DISKS[@]}"; do
                if [ "${AVAILABLE_DISKS[$i]}" = "$disk" ]; then
                    echo -e "  - $disk: ${DISK_RAID_STATUS[$i]}"
                    break
                fi
            done
        done
        
        show_warning "Â¡TODOS LOS DATOS EN ESTOS DISCOS SE PERDERÃN PERMANENTEMENTE!"
        
        if ! confirm "Â¿EstÃ¡s completamente seguro de que deseas continuar?"; then
            show_message "OperaciÃ³n cancelada por el usuario"
            exit 0
        fi
        
        # Proceder con la limpieza
        for disk in "${DISKS_TO_CLEAN[@]}"; do
            clean_disk "$disk"
        done
        
        show_message "Todos los discos han sido limpiados exitosamente"
    fi
    
    # Calcular capacidad del RAID
    calculate_raid_capacity "$RAID_TYPE" "${SELECTED_DISK_SIZES[@]}"
}

# FunciÃ³n para configurar BTRFS
setup_btrfs() {
    show_title "Configurando BTRFS RAID"
    
    # Seleccionar tipo de RAID
    show_raid_types
    while true; do
        read -p "Selecciona el tipo de RAID (1-5): " choice
        case $choice in
            1) RAID_TYPE="raid0"; break;;
            2) RAID_TYPE="raid1"; break;;
            3) RAID_TYPE="raid5"; 
               show_warning "RAID 5 es experimental en BTRFS"
               if confirm "Â¿Deseas continuar?"; then break; else continue; fi;;
            4) RAID_TYPE="raid6";
               show_warning "RAID 6 es experimental en BTRFS"
               if confirm "Â¿Deseas continuar?"; then break; else continue; fi;;
            5) RAID_TYPE="raid10"; break;;
            *) echo "OpciÃ³n invÃ¡lida";;
        esac
    done
    
    select_disks
    
    # Solicitar punto de montaje
    read -p "Ingresa la ruta donde quieres montar el RAID (ej: /mnt/raid): " MOUNT_POINT
    
    if [ ! -d "$MOUNT_POINT" ]; then
        if confirm "El directorio $MOUNT_POINT no existe. Â¿Deseas crearlo?"; then
            sudo mkdir -p "$MOUNT_POINT"
            show_message "Directorio $MOUNT_POINT creado"
        else
            show_error "Se necesita un punto de montaje vÃ¡lido"
            exit 1
        fi
    fi
    
    # Crear el filesystem BTRFS
    show_message "Creando filesystem BTRFS..."
    
    device_list=""
    for disk in "${SELECTED_DISKS[@]}"; do
        device_list="$device_list /dev/$disk"
    done
    
    sudo mkfs.btrfs -f -d "$RAID_TYPE" -m "$RAID_TYPE" $device_list
    
    # Montar el filesystem
    sudo mount "/dev/${SELECTED_DISKS[0]}" "$MOUNT_POINT"
    
    show_message "BTRFS RAID configurado exitosamente"
    FILESYSTEM_UUID=$(sudo blkid -s UUID -o value "/dev/${SELECTED_DISKS[0]}")
}

# FunciÃ³n para obtener RAM del sistema
get_system_ram() {
    local ram_kb=$(grep MemTotal /proc/meminfo | awk '{print $2}')
    local ram_gb=$((ram_kb / 1024 / 1024))
    echo $ram_gb
}

# FunciÃ³n para configurar ZFS
setup_zfs() {
    show_title "Configurando ZFS RAID"
    
    # Verificar que ZFS estÃ© disponible (deberÃ­a estarlo despuÃ©s de check_and_install_requirements)
    if ! command -v zpool &> /dev/null; then
        show_error "ZFS no estÃ¡ disponible. Esto no deberÃ­a suceder despuÃ©s de la verificaciÃ³n de requisitos."
        exit 1
    fi
    
    # Verificar que el mÃ³dulo ZFS estÃ© cargado
    if ! lsmod | grep -q "^zfs "; then
        show_message "Cargando mÃ³dulo ZFS..."
        sudo modprobe zfs
        sleep 2
        
        if ! lsmod | grep -q "^zfs "; then
            show_error "No se pudo cargar el mÃ³dulo ZFS"
            exit 1
        fi
    fi
    
    # Seleccionar tipo de RAID
    show_raid_types
    while true; do
        read -p "Selecciona el tipo de RAID (1-5): " choice
        case $choice in
            1) RAID_TYPE="stripe"; break;;
            2) RAID_TYPE="mirror"; break;;
            3) RAID_TYPE="raidz1"; break;;
            4) RAID_TYPE="raidz2"; break;;
            5) RAID_TYPE="raidz3"; break;;
            *) echo "OpciÃ³n invÃ¡lida";;
        esac
    done
    
    select_disks
    
    # Solicitar nombre del pool
    read -p "Ingresa el nombre del pool ZFS: " POOL_NAME
    
    # Configurar ARC
    local system_ram=$(get_system_ram)
    local recommended_arc=$((system_ram / 4))
    if [ $recommended_arc -lt 1 ]; then
        recommended_arc=1
    fi
    
    show_message "RAM del sistema: ${system_ram}GB"
    show_message "ARC recomendado: ${recommended_arc}GB"
    
    read -p "Â¿CuÃ¡ntos GB quieres asignar al ARC? [$recommended_arc]: " arc_size
    arc_size=${arc_size:-$recommended_arc}
    
    # Detectar ashift Ã³ptimo considerando compatibilidad futura
    local optimal_ashift=12  # Default para compatibilidad con cache devices
    local max_sector_size=512
    local has_4k_sectors=false
    
    # Detectar el tamaÃ±o de sector mÃ¡s grande entre los discos seleccionados
    for disk in "${SELECTED_DISKS[@]}"; do
        if [ -b "/dev/$disk" ]; then
            local sector_size=$(sudo blockdev --getpbsz "/dev/$disk" 2>/dev/null)
            if [ -n "$sector_size" ]; then
                if [ "$sector_size" -gt "$max_sector_size" ]; then
                    max_sector_size=$sector_size
                fi
                if [ "$sector_size" -eq 4096 ]; then
                    has_4k_sectors=true
                fi
            fi
        fi
    done
    
    # Estrategia de ashift optimizada para compatibilidad
    if [ "$max_sector_size" -le 512 ] && [ "$has_4k_sectors" = false ]; then
        # Solo discos 512-byte: usar ashift=12 por compatibilidad con cache futuro
        optimal_ashift=12
        show_message "[INFO] ğŸ”§ Usando ashift=12 para compatibilidad con cache devices SSD"
    else
        # Calcular ashift basado en el tamaÃ±o de sector mÃ¡s grande
        case $max_sector_size in
            512) optimal_ashift=12 ;;     # Forzar 12 para compatibilidad
            1024) optimal_ashift=10 ;;    # 2^10 = 1024 bytes  
            2048) optimal_ashift=11 ;;    # 2^11 = 2048 bytes
            4096) optimal_ashift=12 ;;    # 2^12 = 4096 bytes (Ã³ptimo)
            8192) optimal_ashift=13 ;;    # 2^13 = 8192 bytes
            *) optimal_ashift=12 ;;       # Default seguro
        esac
    fi
    
    show_message "[INFO] ğŸ“Š ConfiguraciÃ³n de sectores detectada:"
    show_message "[INFO]    TamaÃ±o de sector mÃ¡ximo en HDDs: ${max_sector_size} bytes"
    show_message "[INFO]    Ashift del pool: ${optimal_ashift} (2^${optimal_ashift} = $((2**optimal_ashift)) bytes)"
    show_message "[INFO]    âœ… Esto garantiza compatibilidad con cache devices SSD (4096 bytes)"
    
    # Crear el pool ZFS
    show_message "Creando pool ZFS..."
    
    device_list=""
    for disk in "${SELECTED_DISKS[@]}"; do
        device_list="$device_list /dev/$disk"
    done
    
    case $RAID_TYPE in
        "stripe")
            sudo zpool create -o ashift=$optimal_ashift "$POOL_NAME" $device_list
            ;;
        "mirror")
            sudo zpool create -o ashift=$optimal_ashift "$POOL_NAME" mirror $device_list
            ;;
        "raidz1")
            sudo zpool create -o ashift=$optimal_ashift "$POOL_NAME" raidz $device_list
            ;;
        "raidz2")
            sudo zpool create -o ashift=$optimal_ashift "$POOL_NAME" raidz2 $device_list
            ;;
        "raidz3")
            sudo zpool create -o ashift=$optimal_ashift "$POOL_NAME" raidz3 $device_list
            ;;
    esac
    
    # Configurar ARC
    echo $((arc_size * 1024 * 1024 * 1024)) | sudo tee /sys/module/zfs/parameters/zfs_arc_max > /dev/null
    
    # Hacer persistente la configuraciÃ³n del ARC
    sudo mkdir -p /etc/modprobe.d
    echo "options zfs zfs_arc_max=$((arc_size * 1024 * 1024 * 1024))" | sudo tee /etc/modprobe.d/zfs.conf > /dev/null
    
    MOUNT_POINT="/$POOL_NAME"
    
    # Configurar NVME para cache y log si estÃ¡ disponible
    if [ -n "$NVME_DISK" ] && [[ ! " ${SELECTED_DISKS[*]} " =~ " ${NVME_DISK} " ]]; then
        if confirm "Â¿Deseas usar el NVME ($NVME_DISK) para ARC2 (L2ARC) y SLOG?"; then
            show_message "Particionando NVME para cache y log..."
            
            # Crear particiones en el NVME
            sudo parted "/dev/$NVME_DISK" --script mklabel gpt
            sudo parted "/dev/$NVME_DISK" --script mkpart primary 0% 50%
            sudo parted "/dev/$NVME_DISK" --script mkpart primary 50% 100%
            
            sleep 2
            
            # Agregar cache y log al pool
            sudo zpool add "$POOL_NAME" cache "/dev/${NVME_DISK}p1"
            sudo zpool add "$POOL_NAME" log "/dev/${NVME_DISK}p2"
            
            show_message "NVME configurado como cache y log"
        fi
    fi
    
    show_message "ZFS pool configurado exitosamente"
    
    # Crear datasets dentro del pool principal usando la funciÃ³n reutilizable
    echo ""
    if confirm "Â¿Deseas crear datasets dentro del pool '$POOL_NAME'?"; then
        create_datasets_in_pool "$POOL_NAME"
    fi
    
    # Preguntar por pools adicionales solo si hay discos restantes
    remaining_disks=()
    for disk in "${AVAILABLE_DISKS[@]}"; do
        if [[ ! " ${SELECTED_DISKS[*]} " =~ " ${disk} " ]] && [ "$disk" != "$NVME_DISK" ]; then
            # Verificar que el disco no sea demasiado pequeÃ±o (ignorar mmcblk0boot)
            if [[ ! "$disk" =~ mmcblk.*boot ]]; then
                remaining_disks+=("$disk")
            fi
        fi
    done
    
    if [ ${#remaining_disks[@]} -gt 0 ]; then
        echo ""
        show_message "Discos disponibles restantes:"
        for disk in "${remaining_disks[@]}"; do
            size=$(lsblk -dpno SIZE "/dev/$disk" | tr -d ' ')
            model=$(lsblk -dpno MODEL "/dev/$disk" | tr -d ' ')
            echo "  - $disk ($size, $model)"
        done
        
        echo ""
        echo "ğŸ’¡ INFORMACIÃ“N SOBRE POOLS ADICIONALES:"
        echo "   Los pools adicionales son independientes del pool principal."
        echo "   Ãštiles para:"
        echo "   â€¢ Separar datos por uso (backup, cache, temp)"
        echo "   â€¢ Diferentes niveles de redundancia"
        echo "   â€¢ Optimizar rendimiento por tipo de carga"
        echo ""
        echo "   Ejemplo: Pool principal para datos + Pool SSD para cache"
        echo ""
        
        if confirm "Â¿Deseas crear pools ZFS adicionales con los discos restantes?"; then
            # Implementar creaciÃ³n de pools adicionales
            while [ ${#remaining_disks[@]} -gt 0 ]; do
                echo ""
                show_title "ConfiguraciÃ³n de Pool Adicional"
                
                read -p "Nombre del nuevo pool: " additional_pool_name
                
                if [ -z "$additional_pool_name" ]; then
                    show_error "El nombre del pool no puede estar vacÃ­o"
                    continue
                fi
                
                # Verificar que el nombre no exista
                if zpool list -H -o name 2>/dev/null | grep -q "^${additional_pool_name}$"; then
                    show_error "Ya existe un pool con el nombre '$additional_pool_name'"
                    continue
                fi
                
                echo "Discos disponibles para el pool '$additional_pool_name':"
                for i in "${!remaining_disks[@]}"; do
                    disk="${remaining_disks[$i]}"
                    size=$(lsblk -dpno SIZE "/dev/$disk" | tr -d ' ')
                    model=$(lsblk -dpno MODEL "/dev/$disk" | tr -d ' ')
                    echo "  $((i+1)). $disk - $size - $model"
                done
                
                echo ""
                echo "Tipos de RAID disponibles:"
                echo "1. Stripe - Sin redundancia, mÃ¡ximo rendimiento"
                echo "2. Mirror - Datos duplicados entre discos"
                echo "3. RAIDZ1 - Un disco de paridad (mÃ­nimo 3 discos)"
                echo "4. RAIDZ2 - Dos discos de paridad (mÃ­nimo 4 discos)"
                
                read -p "Selecciona tipo de RAID (1-4): " raid_choice
                
                local additional_raid_type=""
                local min_disks_needed=1
                
                case $raid_choice in
                    1) additional_raid_type="stripe"; min_disks_needed=1;;
                    2) additional_raid_type="mirror"; min_disks_needed=2;;
                    3) additional_raid_type="raidz1"; min_disks_needed=3;;
                    4) additional_raid_type="raidz2"; min_disks_needed=4;;
                    *) 
                        show_error "OpciÃ³n invÃ¡lida"
                        continue
                        ;;
                esac
                
                if [ ${#remaining_disks[@]} -lt $min_disks_needed ]; then
                    show_error "Se necesitan al menos $min_disks_needed discos para $additional_raid_type"
                    continue
                fi
                
                # SelecciÃ³n simple para pools adicionales
                selected_additional_disks=()
                echo ""
                echo "Selecciona discos para '$additional_pool_name' (mÃ­nimo $min_disks_needed):"
                echo "Ingresa nÃºmeros separados por espacios (ej: 1 2 3) o 'all' para todos:"
                
                read -p "SelecciÃ³n: " disk_selection
                
                if [ "$disk_selection" = "all" ]; then
                    selected_additional_disks=("${remaining_disks[@]}")
                else
                    for num in $disk_selection; do
                        if [[ "$num" =~ ^[0-9]+$ ]] && [ "$num" -ge 1 ] && [ "$num" -le ${#remaining_disks[@]} ]; then
                            idx=$((num-1))
                            selected_additional_disks+=("${remaining_disks[$idx]}")
                        fi
                    done
                fi
                
                if [ ${#selected_additional_disks[@]} -lt $min_disks_needed ]; then
                    show_error "Se necesitan al menos $min_disks_needed discos"
                    continue
                fi
                
                # Crear el pool adicional
                show_message "Creando pool adicional '$additional_pool_name'..."
                
                # Detectar ashift Ã³ptimo para el pool adicional
                local additional_optimal_ashift=12  # Default para compatibilidad
                local additional_max_sector_size=512
                local additional_has_4k_sectors=false
                
                # Detectar el tamaÃ±o de sector mÃ¡s grande entre los discos seleccionados
                for disk in "${selected_additional_disks[@]}"; do
                    if [ -b "/dev/$disk" ]; then
                        local sector_size=$(sudo blockdev --getpbsz "/dev/$disk" 2>/dev/null)
                        if [ -n "$sector_size" ]; then
                            if [ "$sector_size" -gt "$additional_max_sector_size" ]; then
                                additional_max_sector_size=$sector_size
                            fi
                            if [ "$sector_size" -eq 4096 ]; then
                                additional_has_4k_sectors=true
                            fi
                        fi
                    fi
                done
                
                # Estrategia de ashift optimizada para compatibilidad
                if [ "$additional_max_sector_size" -le 512 ] && [ "$additional_has_4k_sectors" = false ]; then
                    # Solo discos 512-byte: usar ashift=12 por compatibilidad
                    additional_optimal_ashift=12
                    show_message "[INFO] ğŸ”§ Usando ashift=12 para compatibilidad con cache devices"
                else
                    # Calcular ashift basado en el tamaÃ±o de sector mÃ¡s grande
                    case $additional_max_sector_size in
                        512) additional_optimal_ashift=12 ;;     # Forzar 12 para compatibilidad
                        1024) additional_optimal_ashift=10 ;;
                        2048) additional_optimal_ashift=11 ;;
                        4096) additional_optimal_ashift=12 ;;
                        8192) additional_optimal_ashift=13 ;;
                        *) additional_optimal_ashift=12 ;;
                    esac
                fi
                
                show_message "[INFO] ğŸ“Š ConfiguraciÃ³n para pool '$additional_pool_name':"
                show_message "[INFO]    TamaÃ±o de sector mÃ¡ximo en HDDs: ${additional_max_sector_size} bytes" 
                show_message "[INFO]    Ashift del pool: ${additional_optimal_ashift} (2^${additional_optimal_ashift} = $((2**additional_optimal_ashift)) bytes)"
                show_message "[INFO]    âœ… Garantiza compatibilidad con cache devices futuro"
                
                # Limpiar discos seleccionados antes de crear el pool
                for disk in "${selected_additional_disks[@]}"; do
                    show_message "ğŸ§¹ Limpiando disco /dev/$disk antes de usarlo..."
                    clean_disk "$disk"
                done
                
                local device_list=""
                for disk in "${selected_additional_disks[@]}"; do
                    device_list="$device_list /dev/$disk"
                done
                
                case $additional_raid_type in
                    "stripe")
                        sudo zpool create -o ashift=$additional_optimal_ashift "$additional_pool_name" $device_list
                        ;;
                    "mirror")
                        sudo zpool create -o ashift=$additional_optimal_ashift "$additional_pool_name" mirror $device_list
                        ;;
                    "raidz1")
                        sudo zpool create -o ashift=$additional_optimal_ashift "$additional_pool_name" raidz $device_list
                        ;;
                    "raidz2")
                        sudo zpool create -o ashift=$additional_optimal_ashift "$additional_pool_name" raidz2 $device_list
                        ;;
                esac
                
                if [ $? -eq 0 ]; then
                    show_message "âœ… Pool '$additional_pool_name' creado exitosamente"
                    
                    # Remover discos usados de la lista de disponibles
                    new_remaining=()
                    for disk in "${remaining_disks[@]}"; do
                        if [[ ! " ${selected_additional_disks[*]} " =~ " ${disk} " ]]; then
                            new_remaining+=("$disk")
                        fi
                    done
                    remaining_disks=("${new_remaining[@]}")
                    
                    if [ ${#remaining_disks[@]} -eq 0 ]; then
                        show_message "No quedan mÃ¡s discos disponibles"
                        break
                    fi
                    
                    if ! confirm "Â¿Crear otro pool adicional?"; then
                        break
                    fi
                else
                    show_error "âŒ Error creando el pool '$additional_pool_name'"
                fi
            done
        fi
    else
        show_message "â„¹ï¸  No hay discos adicionales disponibles para mÃ¡s pools"
    fi
}

# FunciÃ³n para mostrar resumen
show_summary() {
    show_title "Resumen de ConfiguraciÃ³n"
    
    echo "Sistema de archivos: $FILESYSTEM_TYPE"
    echo "Tipo de RAID: $RAID_TYPE"
    echo "Discos utilizados: ${SELECTED_DISKS[*]}"
    echo "Punto de montaje: $MOUNT_POINT"
    
    if [ "$FILESYSTEM_TYPE" = "zfs" ]; then
        echo "Nombre del pool: $POOL_NAME"
        echo "ARC configurado: ${arc_size}GB"
    fi
    
    echo "Capacidad aproximada del RAID: $(( RAID_CAPACITY / 1024 / 1024 / 1024 )) GB"
}

# FunciÃ³n para configurar montaje automÃ¡tico
setup_auto_mount() {
    if confirm "Â¿Deseas montar automÃ¡ticamente al iniciar el sistema?"; then
        if [ "$FILESYSTEM_TYPE" = "btrfs" ]; then
            # Agregar entrada a /etc/fstab para BTRFS
            echo "UUID=$FILESYSTEM_UUID $MOUNT_POINT btrfs defaults 0 0" | sudo tee -a /etc/fstab
            show_message "Montaje automÃ¡tico configurado en /etc/fstab"
        else
            # Para ZFS, habilitar el servicio de importaciÃ³n automÃ¡tica
            sudo systemctl enable zfs-import-cache
            sudo systemctl enable zfs-mount
            sudo systemctl enable zfs.target
            show_message "Servicios ZFS habilitados para montaje automÃ¡tico"
        fi
    fi
}

# FunciÃ³n principal
main() {
    show_title "Script de ConfiguraciÃ³n RAID para Raspberry Pi"
    
    # Verificar si se ejecuta como root o con sudo
    if [ "$EUID" -eq 0 ]; then
        show_error "No ejecutes este script como root. Usa sudo cuando sea necesario."
        exit 1
    fi
    
    # Verificar e instalar requisitos
    check_and_install_requirements
    
    # Detectar configuraciones RAID existentes (ZFS, BTRFS, MDADM, LVM)
    if detect_existing_raid_configurations; then
        # ContinÃºa con configuraciÃ³n normal
        detect_disks
        select_filesystem
        
        if [ "$FILESYSTEM_TYPE" = "btrfs" ]; then
            setup_btrfs
        else
            setup_zfs
        fi
        
        show_summary
        setup_auto_mount
        
        show_message "Â¡ConfiguraciÃ³n RAID completada exitosamente!"
        show_message "El RAID estÃ¡ montado en: $MOUNT_POINT"
        
        # Ofrecer gestiÃ³n de datasets despuÃ©s de crear el pool
        offer_post_creation_dataset_management
    fi
    # Si detectÃ³ configuraciones RAID existentes y el usuario eligiÃ³ gestionarlas, 
    # la funciÃ³n detect_existing_raid_configurations ya manejÃ³ todo
}

# FunciÃ³n para ofrecer gestiÃ³n de datasets despuÃ©s de crear un pool
offer_post_creation_dataset_management() {
    if [ "$FILESYSTEM_TYPE" = "zfs" ] && [ -n "$POOL_NAME" ]; then
        echo ""
        show_title "GestiÃ³n Post-CreaciÃ³n"
        echo ""
        echo "ğŸ‰ Â¡Pool ZFS '$POOL_NAME' creado exitosamente!"
        echo ""
        
        # Configurar atime primero (recomendado)
        configure_atime_settings "$POOL_NAME"
        
        while true; do
            echo ""
            echo "ğŸ› ï¸  OPCIONES ADICIONALES:"
            echo "   1. ğŸš€ Configurar dispositivos de cache (L2ARC/SLOG) - RECOMENDADO"
            echo "   2. ğŸ“ Gestionar datasets en '$POOL_NAME'"
            echo "   3. ğŸ“¸ Crear snapshots iniciales"
            echo "   4. ğŸ“Š Ver estado completo del pool"
            echo "   5. âš™ï¸  Reconfigurar atime"
            echo "   6. âœ… Finalizar"
            echo ""
            echo "ğŸ’¡ NOTA: Los dispositivos de cache (opciÃ³n 1) deben ser SSD o NVMe"
            echo "   âš ï¸  NO usar discos mecÃ¡nicos - pueden empeorar el rendimiento"
            echo ""
            
            read -p "ğŸ‘‰ Selecciona una opciÃ³n (1-6): " choice
            case $choice in
                1)
                    echo ""
                    setup_cache_devices "$POOL_NAME"
                    ;;
                2)
                    echo ""
                    create_datasets_in_pool "$POOL_NAME"
                    ;;
                3)
                    create_initial_snapshots "$POOL_NAME"
                    ;;
                4)
                    if [ -z "$POOL_NAME" ]; then
                        show_error "Error: POOL_NAME no estÃ¡ definido. Esto no deberÃ­a ocurrir."
                        echo "InformaciÃ³n de debug:"
                        echo "  FILESYSTEM_TYPE: $FILESYSTEM_TYPE"
                        echo "  Pools disponibles: $(zpool list -H -o name 2>/dev/null | tr '\n' ' ')"
                        read -p "Presiona Enter para continuar..."
                    else
                        show_pool_status "$POOL_NAME"
                    fi
                    ;;
                5)
                    echo ""
                    configure_atime_settings "$POOL_NAME"
                    ;;
                6)
                    show_message "Â¡ConfiguraciÃ³n completada!"
                    echo ""
                    echo "ğŸ“Š RESUMEN FINAL:"
                    echo "   Pool ZFS: $POOL_NAME"
                    echo "   Estado: $(zpool list -H -o health "$POOL_NAME" 2>/dev/null || echo 'Desconocido')"
                    echo "   TamaÃ±o: $(zpool list -H -o size "$POOL_NAME" 2>/dev/null || echo 'Desconocido')"
                    echo "   Usado: $(zpool list -H -o alloc "$POOL_NAME" 2>/dev/null || echo 'Desconocido')"
                    echo "   Libre: $(zpool list -H -o free "$POOL_NAME" 2>/dev/null || echo 'Desconocido')"
                    echo ""
                    echo "âœ… ConfiguraciÃ³n de RAID ZFS completada exitosamente!"
                    break
                    ;;
                *)
                    echo "âŒ OpciÃ³n invÃ¡lida. Selecciona 1, 2, 3, 4, 5 o 6."
                    ;;
            esac
            echo ""
        done
    fi
    
    # Asegurar que las variables estÃ©n definidas para evitar errores
    if [ -z "$POOL_NAME" ] && [ "$FILESYSTEM_TYPE" = "zfs" ]; then
        echo "âš ï¸  ADVERTENCIA: POOL_NAME no estÃ¡ definido al final de la gestiÃ³n post-creaciÃ³n."
        echo "   Esto podrÃ­a indicar un problema en el script."
    fi
}

# FunciÃ³n para configurar atime en ZFS
configure_atime_settings() {
    local pool_name="$1"
    
    show_title "ConfiguraciÃ³n de Atime en ZFS"
    echo ""
    echo "ğŸ’¡ INFORMACIÃ“N SOBRE ATIME:"
    echo "   atime (access time) registra la Ãºltima vez que se accediÃ³ a un archivo."
    echo "   En sistemas con muchas operaciones de lectura, puede impactar el rendimiento."
    echo ""
    echo "ğŸ“Š OPCIONES DISPONIBLES EN ZFS:"
    echo "   1. off        - No registrar atime (RECOMENDADO para rendimiento)"
    echo "   2. on         - Registrar atime completo (puede reducir rendimiento)"
    echo ""
    echo "ğŸ’¡ RECOMENDACIÃ“N: OpciÃ³n 1 (off) para mÃ¡ximo rendimiento"
    echo "   âš ï¸  NOTA: ZFS solo soporta 'on' u 'off' (no soporta 'relatime')"
    echo "   La mayorÃ­a de aplicaciones no necesitan atime y deshabilitarlo"
    echo "   mejora significativamente el rendimiento de lectura."
    echo ""
    
    while true; do
        read -p "ğŸ‘‰ Selecciona configuraciÃ³n de atime (1-2) [1]: " atime_choice
        atime_choice=${atime_choice:-1}
        
        case $atime_choice in
            1)
                local atime_setting="off"
                local description="Deshabilitado (mÃ¡ximo rendimiento)"
                break
                ;;
            2)
                local atime_setting="on"
                local description="Habilitado (impacto en rendimiento)"
                break
                ;;
            *)
                echo "âŒ OpciÃ³n invÃ¡lida. Selecciona 1 o 2."
                ;;
        esac
    done
    
    show_message "Configurando atime=$atime_setting en el pool '$pool_name'..."
    
    # Aplicar configuraciÃ³n al pool raÃ­z
    if sudo zfs set atime="$atime_setting" "$pool_name"; then
        show_message "âœ… atime configurado como '$atime_setting' en '$pool_name'"
    else
        show_error "âŒ Error configurando atime en '$pool_name'"
        return 1
    fi
    
    # Aplicar a todos los datasets existentes
    local datasets=$(zfs list -H -o name -r "$pool_name" 2>/dev/null | grep -v "^${pool_name}$")
    if [ -n "$datasets" ]; then
        show_message "Aplicando configuraciÃ³n a datasets existentes..."
        for dataset in $datasets; do
            if sudo zfs set atime="$atime_setting" "$dataset"; then
                show_message "  âœ… $dataset configurado"
            else
                show_warning "  âš ï¸  Error configurando $dataset"
            fi
        done
    fi
    
    echo ""
    show_message "ğŸ“Š CONFIGURACIÃ“N ATIME COMPLETADA:"
    echo "   Pool: $pool_name"
    echo "   ConfiguraciÃ³n: $atime_setting ($description)"
    echo "   Aplicado a: pool y todos los datasets existentes"
    echo ""
    echo "ğŸ’¡ Nota: Los nuevos datasets heredarÃ¡n esta configuraciÃ³n automÃ¡ticamente"
}

# FunciÃ³n para configurar dispositivos de cache (L2ARC y SLOG)
setup_cache_devices() {
    local pool_name="$1"
    
    show_title "ConfiguraciÃ³n de Dispositivos de Cache para '$pool_name'"
    echo ""
    echo "âš ï¸  ADVERTENCIA CRÃTICA SOBRE DISPOSITIVOS DE CACHE:"
    echo "   ğŸ”¥ SOLO USAR SSD O NVME - NUNCA DISCOS MECÃNICOS"
    echo "   ğŸš« Los discos mecÃ¡nicos (HDD) como cache EMPEORARÃN el rendimiento"
    echo "   âœ… Los dispositivos de cache deben ser MÃS RÃPIDOS que el almacenamiento principal"
    echo ""
    echo "ğŸ’¡ INFORMACIÃ“N SOBRE DISPOSITIVOS DE CACHE:"
    echo ""
    echo "ğŸš€ L2ARC (Level 2 Adaptive Replacement Cache):"
    echo "   â€¢ Cache de segundo nivel para lecturas frecuentes"
    echo "   â€¢ Ideal: SSD rÃ¡pido (NVMe > SATA SSD > USB 3.0)"
    echo "   â€¢ Mejora rendimiento de lectura en datasets accedidos frecuentemente"
    echo "   â€¢ No es crÃ­tico - si falla, el pool sigue funcionando"
    echo ""
    echo "ğŸ“ SLOG (Separate Intent Log):"
    echo "   â€¢ Log de transacciones para escrituras sÃ­ncronas"
    echo "   â€¢ Ideal: SSD con baja latencia y alta resistencia (NVMe con power-loss protection)"
    echo "   â€¢ Mejora rendimiento de escrituras sÃ­ncronas (bases de datos, VMs)"
    echo "   â€¢ CrÃ­tico para integridad - usar dispositivos confiables"
    echo ""
    echo "âš ï¸  RECOMENDACIÃ“N IMPORTANTE:"
    echo "   ğŸ”¥ USAR DISPOSITIVOS NVME ES ALTAMENTE RECOMENDADO PARA CACHE"
    echo "   â€¢ NVMe ofrece la latencia mÃ¡s baja y mayor throughput"
    echo "   â€¢ Usar dispositivos lentos (USB, HDD) como cache puede EMPEORAR el rendimiento"
    echo "   â€¢ Si no tienes NVMe disponible, considera omitir la configuraciÃ³n de cache"
    echo ""
    
    # Detectar dispositivos disponibles con clasificaciÃ³n por tipo
    local pool_devices=$(zpool status "$pool_name" 2>/dev/null | grep -E "^\s+[a-z]" | awk '{print $1}' | grep -v "raidz\|mirror\|spare\|log\|cache\|replacing")
    local available_cache_devices=()
    local nvme_devices=()
    local ssd_devices=()
    local other_devices=()
    
    # Obtener lista de todos los discos disponibles y clasificarlos
    local available_cache_devices=()
    local nvme_devices=()
    local ssd_devices=()
    local other_devices=()
    local in_use_devices=()
    
    while IFS= read -r disk; do
        local disk_name=$(basename "$disk")
        
        # Excluir dispositivos del sistema y de arranque
        if [[ "$disk_name" == "mmcblk0"* ]] || [[ "$disk_name" == *"boot"* ]]; then
            continue
        fi
        
        # Verificar que el dispositivo tenga tamaÃ±o vÃ¡lido
        local size_raw=$(lsblk -dpno SIZE "/dev/$disk_name" 2>/dev/null | tr -d ' ')
        if [[ "$size_raw" == "0B" ]] || [[ -z "$size_raw" ]]; then
            continue
        fi
        
        # Verificar que no sea el disco del sistema operativo
        local mount_check=$(lsblk -no MOUNTPOINT "/dev/$disk_name" 2>/dev/null | grep -E "^/$|^/boot|^/home")
        if [ -n "$mount_check" ]; then
            continue
        fi
        
        # Excluir discos ya en uso por el pool principal
        if echo "$pool_devices" | grep -q "^${disk_name}$"; then
            continue
        fi
        
        # Verificar estado de uso del dispositivo
        local raid_status=$(check_disk_raid_status "$disk_name")
        local usage_status=""
        
        if [ -n "$raid_status" ]; then
            # Dispositivo en uso - agregar a lista de dispositivos en uso
            usage_status="EN_USO"
            in_use_devices+=("$disk_name:$raid_status")
        else
            # Dispositivo disponible
            usage_status="DISPONIBLE"
        fi
        
        # Agregar a la lista general y clasificar por tipo
        available_cache_devices+=("$disk_name:$usage_status")
        
        if [[ "$disk_name" == nvme* ]]; then
            nvme_devices+=("$disk_name:$usage_status")
        elif lsblk -dpno ROTA "/dev/$disk_name" 2>/dev/null | grep -q "0"; then
            # ROTA=0 significa SSD
            ssd_devices+=("$disk_name:$usage_status")
        else
            other_devices+=("$disk_name:$usage_status")
        fi
        
    done < <(lsblk -dpno NAME | grep -v "^/dev/loop")
    
    # Mostrar dispositivos disponibles clasificados por tipo
    echo "ğŸ“€ DISPOSITIVOS PARA CACHE:"
    echo ""
    
    if [ ${#nvme_devices[@]} -gt 0 ]; then
        echo "ğŸš€ DISPOSITIVOS NVME (RECOMENDADOS):"
        for device_info in "${nvme_devices[@]}"; do
            local device_name=$(echo "$device_info" | cut -d: -f1)
            local usage_status=$(echo "$device_info" | cut -d: -f2)
            local size=$(lsblk -dpno SIZE "/dev/$device_name" 2>/dev/null | tr -d ' ')
            local model=$(lsblk -dpno MODEL "/dev/$device_name" 2>/dev/null | tr -d ' ')
            
            if [ "$usage_status" = "DISPONIBLE" ]; then
                echo "   âœ… $device_name - $size - $model"
            else
                echo "   âš ï¸  $device_name - $size - $model (EN USO: $usage_status - se puede limpiar)"
            fi
        done
        echo ""
    fi
    
    if [ ${#ssd_devices[@]} -gt 0 ]; then
        echo "ğŸ’¾ DISPOSITIVOS SSD (ACEPTABLES):"
        for device_info in "${ssd_devices[@]}"; do
            local device_name=$(echo "$device_info" | cut -d: -f1)
            local usage_status=$(echo "$device_info" | cut -d: -f2)
            local size=$(lsblk -dpno SIZE "/dev/$device_name" 2>/dev/null | tr -d ' ')
            local model=$(lsblk -dpno MODEL "/dev/$device_name" 2>/dev/null | tr -d ' ')
            
            if [ "$usage_status" = "DISPONIBLE" ]; then
                echo "   âš ï¸  $device_name - $size - $model"
            else
                echo "   âš ï¸  $device_name - $size - $model (EN USO: $usage_status - se puede limpiar)"
            fi
        done
        echo ""
    fi
    
    if [ ${#other_devices[@]} -gt 0 ]; then
        echo "ğŸŒ OTROS DISPOSITIVOS (NO RECOMENDADOS PARA CACHE):"
        for device_info in "${other_devices[@]}"; do
            local device_name=$(echo "$device_info" | cut -d: -f1)
            local usage_status=$(echo "$device_info" | cut -d: -f2)
            local size=$(lsblk -dpno SIZE "/dev/$device_name" 2>/dev/null | tr -d ' ')
            local model=$(lsblk -dpno MODEL "/dev/$device_name" 2>/dev/null | tr -d ' ')
            
            if [ "$usage_status" = "DISPONIBLE" ]; then
                echo "   âŒ $device_name - $size - $model"
            else
                echo "   âŒ $device_name - $size - $model (EN USO: $usage_status - se puede limpiar)"
            fi
        done
        echo ""
    fi
    
    # Mostrar informaciÃ³n sobre dispositivos en uso
    if [ ${#in_use_devices[@]} -gt 0 ]; then
        echo "ğŸ’¡ INFORMACIÃ“N SOBRE DISPOSITIVOS EN USO:"
        echo "   â€¢ Los dispositivos marcados como 'EN USO' pueden ser limpiados y reutilizados"
        echo "   â€¢ âš ï¸  ADVERTENCIA: Limpiar un dispositivo DESTRUIRÃ todos los datos"
        echo "   â€¢ Se te pedirÃ¡ confirmaciÃ³n antes de proceder con dispositivos en uso"
        echo ""
    fi
    
    # Verificar dispositivos disponibles y mostrar recomendaciÃ³n apropiada
    if [ ${#nvme_devices[@]} -eq 0 ] && [ ${#ssd_devices[@]} -eq 0 ]; then
        # No hay NVMe ni SSD - advertencia crÃ­tica
        echo "âš ï¸  ADVERTENCIA CRÃTICA:"
        echo "   ğŸ”¥ NO SE DETECTARON DISPOSITIVOS NVMe O SSD ADECUADOS"
        echo "   â€¢ Los dispositivos de cache deben ser mÃ¡s rÃ¡pidos que el almacenamiento principal"
        echo "   â€¢ Usar dispositivos lentos como cache puede REDUCIR el rendimiento del sistema"
        echo "   â€¢ Se recomienda FUERTEMENTE conseguir un dispositivo NVMe o SSD para cache"
        echo ""
        
        if ! confirm "Â¿Deseas continuar sin dispositivos rÃ¡pidos (MUY NO RECOMENDADO)?"; then
            show_message "ConfiguraciÃ³n de cache cancelada - se recomienda conseguir un dispositivo NVMe o SSD"
            return 0
        fi
        echo ""
    elif [ ${#nvme_devices[@]} -eq 0 ] && [ ${#ssd_devices[@]} -gt 0 ]; then
        # Solo hay SSD, no NVMe - recomendaciÃ³n suave
        echo "ğŸ’¡ RECOMENDACIÃ“N:"
        echo "   âœ… Se detectaron dispositivos SSD (aceptables para cache)"
        echo "   ğŸš€ Para rendimiento Ã³ptimo, considera usar NVMe en el futuro"
        echo "   ğŸ“Š Los SSD son adecuados para la mayorÃ­a de casos de uso"
        echo ""
    fi
    
    if [ ${#available_cache_devices[@]} -eq 0 ]; then
        show_warning "No se encontraron dispositivos disponibles para cache"
        show_message "Todos los dispositivos estÃ¡n en uso o no son adecuados"
        return 0
    fi
    
    echo "ï¸  OPCIONES DE CONFIGURACIÃ“N:"
    echo "   1. Configurar solo L2ARC (cache de lectura)"
    echo "   2. Configurar solo SLOG (log de escritura)"
    echo "   3. Configurar ambos L2ARC y SLOG"
    echo "   4. Configurar particiones en un dispositivo (L2ARC + SLOG)"
    echo "   5. Cancelar configuraciÃ³n"
    echo ""
    
    while true; do
        read -p "ğŸ‘‰ Selecciona una opciÃ³n (1-5): " config_choice
        case $config_choice in
            1)
                if setup_l2arc_only "$pool_name" "${available_cache_devices[@]}"; then
                    break
                else
                    echo ""
                    echo "ï¸  OPCIONES DE CONFIGURACIÃ“N:"
                    echo "   1. Configurar solo L2ARC (cache de lectura)"
                    echo "   2. Configurar solo SLOG (log de escritura)"
                    echo "   3. Configurar ambos L2ARC y SLOG"
                    echo "   4. Configurar particiones en un dispositivo (L2ARC + SLOG)"
                    echo "   5. Cancelar configuraciÃ³n"
                    echo ""
                fi
                ;;
            2)
                if setup_slog_only "$pool_name" "${available_cache_devices[@]}"; then
                    break
                else
                    echo ""
                    echo "ï¸  OPCIONES DE CONFIGURACIÃ“N:"
                    echo "   1. Configurar solo L2ARC (cache de lectura)"
                    echo "   2. Configurar solo SLOG (log de escritura)"
                    echo "   3. Configurar ambos L2ARC y SLOG"
                    echo "   4. Configurar particiones en un dispositivo (L2ARC + SLOG)"
                    echo "   5. Cancelar configuraciÃ³n"
                    echo ""
                fi
                ;;
            3)
                if setup_l2arc_and_slog_separate "$pool_name" "${available_cache_devices[@]}"; then
                    break
                else
                    echo ""
                    echo "ï¸  OPCIONES DE CONFIGURACIÃ“N:"
                    echo "   1. Configurar solo L2ARC (cache de lectura)"
                    echo "   2. Configurar solo SLOG (log de escritura)"
                    echo "   3. Configurar ambos L2ARC y SLOG"
                    echo "   4. Configurar particiones en un dispositivo (L2ARC + SLOG)"
                    echo "   5. Cancelar configuraciÃ³n"
                    echo ""
                fi
                ;;
            4)
                if setup_partitioned_cache "$pool_name" "${available_cache_devices[@]}"; then
                    break
                else
                    echo ""
                    echo "ï¸  OPCIONES DE CONFIGURACIÃ“N:"
                    echo "   1. Configurar solo L2ARC (cache de lectura)"
                    echo "   2. Configurar solo SLOG (log de escritura)"
                    echo "   3. Configurar ambos L2ARC y SLOG"
                    echo "   4. Configurar particiones en un dispositivo (L2ARC + SLOG)"
                    echo "   5. Cancelar configuraciÃ³n"
                    echo ""
                fi
                ;;
            5)
                show_message "ConfiguraciÃ³n de cache cancelada"
                return 0
                ;;
            *)
                echo "âŒ OpciÃ³n invÃ¡lida. Selecciona 1, 2, 3, 4 o 5."
                echo ""
                echo "ï¸  OPCIONES DE CONFIGURACIÃ“N:"
                echo "   1. Configurar solo L2ARC (cache de lectura)"
                echo "   2. Configurar solo SLOG (log de escritura)"
                echo "   3. Configurar ambos L2ARC y SLOG"
                echo "   4. Configurar particiones en un dispositivo (L2ARC + SLOG)"
                echo "   5. Cancelar configuraciÃ³n"
                echo ""
                ;;
        esac
    done
}

# FunciÃ³n para configurar solo L2ARC
setup_l2arc_only() {
    local pool_name="$1"
    shift
    local available_devices=("$@")
    
    show_title "ConfiguraciÃ³n de L2ARC para '$pool_name'"
    echo ""
    echo "ğŸ“– Selecciona el dispositivo para L2ARC (cache de lectura):"
    
    for i in "${!available_devices[@]}"; do
        local device_info="${available_devices[$i]}"
        local device_name=$(echo "$device_info" | cut -d: -f1)
        local usage_status=$(echo "$device_info" | cut -d: -f2)
        local size=$(lsblk -dpno SIZE "/dev/$device_name" | tr -d ' ')
        local model=$(lsblk -dpno MODEL "/dev/$device_name" | tr -d ' ')
        
        if [ "$usage_status" = "DISPONIBLE" ]; then
            echo "  $((i+1)). $device_name - $size - $model"
        else
            echo "  $((i+1)). $device_name - $size - $model (EN USO: $usage_status)"
        fi
    done
    
    while true; do
        read -p "ğŸ‘‰ Selecciona dispositivo (1-${#available_devices[@]}): " device_choice
        if [[ "$device_choice" =~ ^[0-9]+$ ]] && [ "$device_choice" -ge 1 ] && [ "$device_choice" -le ${#available_devices[@]} ]; then
            local device_info="${available_devices[$((device_choice-1))]}"
            local selected_device=$(echo "$device_info" | cut -d: -f1)
            local usage_status=$(echo "$device_info" | cut -d: -f2)
            
            # Verificar si el dispositivo estÃ¡ en uso
            if [ "$usage_status" != "DISPONIBLE" ]; then
                echo ""
                echo "âš ï¸  ADVERTENCIA: El dispositivo $selected_device estÃ¡ EN USO"
                echo "   ğŸ“ Uso actual: $usage_status"
                echo "   ğŸ’€ Limpiar este dispositivo DESTRUIRÃ todos los datos"
                echo ""
                if ! confirm "Â¿EstÃ¡s SEGURO de que quieres limpiar y usar $selected_device para L2ARC?"; then
                    echo "Selecciona otro dispositivo..."
                    continue
                fi
            fi
            
            # Verificar si es un disco mecÃ¡nico
            if ! lsblk -dpno ROTA "/dev/$selected_device" 2>/dev/null | grep -q "0"; then
                echo ""
                echo "ğŸš¨ ADVERTENCIA: El dispositivo $selected_device parece ser un DISCO MECÃNICO"
                echo "   âŒ Los discos mecÃ¡nicos NO deben usarse como cache"
                echo "   ğŸ“‰ Esto EMPEORARÃ significativamente el rendimiento del sistema"
                echo "   âœ… Usa solo SSD o NVMe para cache"
                echo ""
                if ! confirm "Â¿EstÃ¡s SEGURO de que quieres usar este disco mecÃ¡nico? (MUY NO RECOMENDADO)"; then
                    echo "Selecciona otro dispositivo..."
                    continue
                fi
                echo ""
                if ! confirm "âš ï¸  ÃšLTIMA ADVERTENCIA: Â¿Realmente quieres proceder con un disco mecÃ¡nico?"; then
                    echo "Selecciona otro dispositivo..."
                    continue
                fi
            fi
            break
        else
            echo "âŒ SelecciÃ³n invÃ¡lida. Usa nÃºmeros del 1 al ${#available_devices[@]}."
        fi
    done
    
    # Confirmar y aplicar
    echo ""
    show_warning "âš ï¸  Esto limpiarÃ¡ completamente el dispositivo /dev/$selected_device"
    if confirm "Â¿Continuar con la configuraciÃ³n de L2ARC en /dev/$selected_device?"; then
        show_message "Preparando dispositivo /dev/$selected_device para L2ARC..."
        
        # Limpiar dispositivo
        clean_disk "$selected_device"
        
        # Agregar como L2ARC
        show_message "Agregando /dev/$selected_device como L2ARC al pool '$pool_name'..."
        if sudo zpool add "$pool_name" cache "/dev/$selected_device"; then
            show_message "âœ… L2ARC configurado exitosamente"
            echo ""
            show_message "ğŸ“Š CONFIGURACIÃ“N L2ARC COMPLETADA:"
            echo "   Pool: $pool_name"
            echo "   Dispositivo L2ARC: /dev/$selected_device"
            echo "   FunciÃ³n: Cache de lectura (mejora acceso a datos frecuentes)"
            echo "   ğŸ’¡ Cache devices no requieren compatibilidad de ashift"
            return 0
        else
            show_error "âŒ Error configurando L2ARC"
            return 1
        fi
    else
        show_message "ConfiguraciÃ³n de L2ARC cancelada"
        return 1
    fi
}

# FunciÃ³n para configurar solo SLOG
setup_slog_only() {
    local pool_name="$1"
    shift
    local available_devices=("$@")
    
    show_title "ConfiguraciÃ³n de SLOG para '$pool_name'"
    echo ""
    echo "âš ï¸  IMPORTANTE - REQUISITOS PARA SLOG:"
    echo "   â€¢ Usar SSD de alta calidad con protecciÃ³n contra cortes de energÃ­a"
    echo "   â€¢ Evitar dispositivos USB o SD para SLOG"
    echo "   â€¢ Preferir NVMe > SATA SSD > otros"
    echo ""
    echo "ğŸ“– Selecciona el dispositivo para SLOG (log de escritura):"
    
    for i in "${!available_devices[@]}"; do
        local device_info="${available_devices[$i]}"
        local device_name=$(echo "$device_info" | cut -d: -f1)
        local usage_status=$(echo "$device_info" | cut -d: -f2)
        local size=$(lsblk -dpno SIZE "/dev/$device_name" | tr -d ' ')
        local model=$(lsblk -dpno MODEL "/dev/$device_name" | tr -d ' ')
        local recommendation=""
        
        if [[ "$device_name" == nvme* ]]; then
            recommendation=" â­ RECOMENDADO"
        elif lsblk -dpno ROTA "/dev/$device_name" | grep -q "0"; then
            recommendation=" âœ… ADECUADO"
        else
            recommendation=" âš ï¸  NO RECOMENDADO (HDD)"
        fi
        
        if [ "$usage_status" = "DISPONIBLE" ]; then
            echo "  $((i+1)). $device_name - $size - $model$recommendation"
        else
            echo "  $((i+1)). $device_name - $size - $model$recommendation (EN USO: $usage_status)"
        fi
    done
    
    while true; do
        read -p "ğŸ‘‰ Selecciona dispositivo (1-${#available_devices[@]}): " device_choice
        if [[ "$device_choice" =~ ^[0-9]+$ ]] && [ "$device_choice" -ge 1 ] && [ "$device_choice" -le ${#available_devices[@]} ]; then
            local device_info="${available_devices[$((device_choice-1))]}"
            local selected_device=$(echo "$device_info" | cut -d: -f1)
            local usage_status=$(echo "$device_info" | cut -d: -f2)
            
            # Verificar si el dispositivo estÃ¡ en uso
            if [ "$usage_status" != "DISPONIBLE" ]; then
                echo ""
                echo "âš ï¸  ADVERTENCIA CRÃTICA: El dispositivo $selected_device estÃ¡ EN USO"
                echo "   ğŸ“ Uso actual: $usage_status"
                echo "   ğŸ’€ Limpiar este dispositivo DESTRUIRÃ todos los datos"
                echo "   ğŸ”§ SLOG es crÃ­tico - asegÃºrate de que puedes perder estos datos"
                echo ""
                if ! confirm "Â¿EstÃ¡s SEGURO de que quieres limpiar y usar $selected_device para SLOG?"; then
                    echo "Selecciona otro dispositivo..."
                    continue
                fi
            fi
            break
        else
            echo "âŒ SelecciÃ³n invÃ¡lida. Usa nÃºmeros del 1 al ${#available_devices[@]}."
        fi
    done
    
    # Verificar si es adecuado para SLOG
    if ! [[ "$selected_device" == nvme* ]] && lsblk -dpno ROTA "/dev/$selected_device" | grep -q "1"; then
        echo ""
        echo "ğŸš¨ ADVERTENCIA CRÃTICA: El dispositivo $selected_device es un DISCO MECÃNICO"
        echo "   âŒ Los discos mecÃ¡nicos NO deben usarse como SLOG"
        echo "   ğŸ’€ Esto puede causar PÃ‰RDIDA DE DATOS y rendimiento EXTREMADAMENTE pobre"
        echo "   ğŸ”§ SLOG requiere dispositivos de baja latencia (SSD/NVMe)"
        echo "   âš¡ Un SLOG lento puede bloquear TODAS las escrituras del sistema"
        echo ""
        if ! confirm "Â¿EstÃ¡s SEGURO de que quieres usar este disco mecÃ¡nico para SLOG? (PELIGROSO)"; then
            show_message "ConfiguraciÃ³n cancelada por el usuario"
            return 0
        fi
        echo ""
        if ! confirm "âš ï¸  ÃšLTIMA ADVERTENCIA: Â¿Realmente quieres proceder? (PUEDE CAUSAR PROBLEMAS GRAVES)"; then
            show_message "ConfiguraciÃ³n cancelada por el usuario"
            return 0
        fi
    fi
    
    # Confirmar y aplicar
    echo ""
    show_warning "âš ï¸  Esto limpiarÃ¡ completamente el dispositivo /dev/$selected_device"
    if confirm "Â¿Continuar con la configuraciÃ³n de SLOG en /dev/$selected_device?"; then
        show_message "Preparando dispositivo /dev/$selected_device para SLOG..."
        
        # Limpiar dispositivo
        clean_disk "$selected_device"
        
        # Agregar como SLOG
        show_message "Agregando /dev/$selected_device como SLOG al pool '$pool_name'..."
        if sudo zpool add "$pool_name" log "/dev/$selected_device"; then
            show_message "âœ… SLOG configurado exitosamente"
            echo ""
            show_message "ğŸ“Š CONFIGURACIÃ“N SLOG COMPLETADA:"
            echo "   Pool: $pool_name"
            echo "   Dispositivo SLOG: /dev/$selected_device"
            echo "   FunciÃ³n: Log de escritura (mejora escrituras sÃ­ncronas)"
            echo "   ğŸ’¡ Cache devices no requieren compatibilidad de ashift"
            return 0
        else
            show_error "âŒ Error configurando SLOG"
            return 1
        fi
    else
        show_message "ConfiguraciÃ³n de SLOG cancelada"
        return 1
    fi
}

# FunciÃ³n para configurar L2ARC y SLOG en dispositivos separados
setup_l2arc_and_slog_separate() {
    local pool_name="$1"
    shift
    local available_devices=("$@")
    
    if [ ${#available_devices[@]} -lt 2 ]; then
        show_error "Se necesitan al menos 2 dispositivos disponibles para configurar L2ARC y SLOG por separado"
        return 1
    fi
    
    show_title "ConfiguraciÃ³n de L2ARC y SLOG Separados para '$pool_name'"
    echo ""
    echo "ğŸ“– Selecciona dispositivos para L2ARC y SLOG:"
    echo ""
    
    # Mostrar dispositivos con recomendaciones
    for i in "${!available_devices[@]}"; do
        local device_info="${available_devices[$i]}"
        local device_name=$(echo "$device_info" | cut -d: -f1)
        local usage_status=$(echo "$device_info" | cut -d: -f2)
        local size=$(lsblk -dpno SIZE "/dev/$device_name" | tr -d ' ')
        local model=$(lsblk -dpno MODEL "/dev/$device_name" | tr -d ' ')
        local recommendation=""
        
        if [[ "$device_name" == nvme* ]]; then
            recommendation=" â­ EXCELENTE para SLOG, BUENO para L2ARC"
        elif lsblk -dpno ROTA "/dev/$device_name" | grep -q "0"; then
            recommendation=" âœ… BUENO para L2ARC, REGULAR para SLOG"
        else
            recommendation=" âš ï¸  SOLO para L2ARC (NO para SLOG)"
        fi
        
        if [ "$usage_status" = "DISPONIBLE" ]; then
            echo "  $((i+1)). $device_name - $size - $model$recommendation"
        else
            echo "  $((i+1)). $device_name - $size - $model$recommendation (EN USO: $usage_status)"
        fi
    done
    
    # Seleccionar dispositivo para L2ARC
    echo ""
    echo "ğŸš€ SELECCIÃ“N DE L2ARC:"
    while true; do
        read -p "ğŸ‘‰ Dispositivo para L2ARC (1-${#available_devices[@]}): " l2arc_choice
        if [[ "$l2arc_choice" =~ ^[0-9]+$ ]] && [ "$l2arc_choice" -ge 1 ] && [ "$l2arc_choice" -le ${#available_devices[@]} ]; then
            local device_info="${available_devices[$((l2arc_choice-1))]}"
            local l2arc_device=$(echo "$device_info" | cut -d: -f1)
            local l2arc_usage=$(echo "$device_info" | cut -d: -f2)
            
            # Verificar si el dispositivo L2ARC estÃ¡ en uso
            if [ "$l2arc_usage" != "DISPONIBLE" ]; then
                echo ""
                echo "âš ï¸  ADVERTENCIA: El dispositivo $l2arc_device estÃ¡ EN USO"
                echo "   ğŸ“ Uso actual: $l2arc_usage"
                echo "   ğŸ’€ Limpiar este dispositivo DESTRUIRÃ todos los datos"
                echo ""
                if ! confirm "Â¿Continuar con $l2arc_device para L2ARC?"; then
                    echo "Selecciona otro dispositivo..."
                    continue
                fi
            fi
            break
        else
            echo "âŒ SelecciÃ³n invÃ¡lida. Usa nÃºmeros del 1 al ${#available_devices[@]}."
        fi
    done
    
    # Seleccionar dispositivo para SLOG (excluyendo el ya seleccionado)
    echo ""
    echo "ğŸ“ SELECCIÃ“N DE SLOG:"
    echo "Dispositivos disponibles (excluyendo $l2arc_device):"
    local slog_devices=()
    for i in "${!available_devices[@]}"; do
        local device_info="${available_devices[$i]}"
        local device_name=$(echo "$device_info" | cut -d: -f1)
        local usage_status=$(echo "$device_info" | cut -d: -f2)
        
        if [ "$device_name" != "$l2arc_device" ]; then
            slog_devices+=("$device_info")
            local size=$(lsblk -dpno SIZE "/dev/$device_name" | tr -d ' ')
            local model=$(lsblk -dpno MODEL "/dev/$device_name" | tr -d ' ')
            local recommendation=""
            
            if [[ "$device_name" == nvme* ]]; then
                recommendation=" â­ RECOMENDADO"
            elif lsblk -dpno ROTA "/dev/$device_name" | grep -q "0"; then
                recommendation=" âœ… ADECUADO"
            else
                recommendation=" âš ï¸  NO RECOMENDADO"
            fi
            
            if [ "$usage_status" = "DISPONIBLE" ]; then
                echo "  ${#slog_devices[@]}. $device_name - $size - $model$recommendation"
            else
                echo "  ${#slog_devices[@]}. $device_name - $size - $model$recommendation (EN USO: $usage_status)"
            fi
        fi
    done
    
    while true; do
        read -p "ğŸ‘‰ Dispositivo para SLOG (1-${#slog_devices[@]}): " slog_choice
        if [[ "$slog_choice" =~ ^[0-9]+$ ]] && [ "$slog_choice" -ge 1 ] && [ "$slog_choice" -le ${#slog_devices[@]} ]; then
            local device_info="${slog_devices[$((slog_choice-1))]}"
            local slog_device=$(echo "$device_info" | cut -d: -f1)
            local slog_usage=$(echo "$device_info" | cut -d: -f2)
            
            # Verificar si el dispositivo SLOG estÃ¡ en uso
            if [ "$slog_usage" != "DISPONIBLE" ]; then
                echo ""
                echo "âš ï¸  ADVERTENCIA CRÃTICA: El dispositivo $slog_device estÃ¡ EN USO"
                echo "   ğŸ“ Uso actual: $slog_usage"
                echo "   ğŸ’€ Limpiar este dispositivo DESTRUIRÃ todos los datos"
                echo "   ğŸ”§ SLOG es crÃ­tico para integridad de datos"
                echo ""
                if ! confirm "Â¿Continuar con $slog_device para SLOG?"; then
                    echo "Selecciona otro dispositivo..."
                    continue
                fi
            fi
            break
        else
            echo "âŒ SelecciÃ³n invÃ¡lida. Usa nÃºmeros del 1 al ${#slog_devices[@]}."
        fi
    done
    
    # Verificar SLOG si es HDD
    if ! [[ "$slog_device" == nvme* ]] && lsblk -dpno ROTA "/dev/$slog_device" | grep -q "1"; then
        show_warning "âš ï¸  ADVERTENCIA: Dispositivo SLOG seleccionado es HDD"
        if ! confirm "Â¿Continuar con HDD para SLOG? (no recomendado)"; then
            show_message "ConfiguraciÃ³n cancelada"
            return 0
        fi
    fi
    
    # Mostrar resumen y confirmar
    echo ""
    show_message "ğŸ“Š RESUMEN DE CONFIGURACIÃ“N:"
    echo "   Pool: $pool_name"
    echo "   L2ARC: /dev/$l2arc_device (cache de lectura)"
    echo "   SLOG: /dev/$slog_device (log de escritura)"
    echo ""
    
    show_warning "âš ï¸  Esto limpiarÃ¡ completamente ambos dispositivos"
    if confirm "Â¿Continuar con la configuraciÃ³n?"; then
        # Configurar L2ARC
        show_message "Preparando /dev/$l2arc_device para L2ARC..."
        clean_disk "$l2arc_device"
        
        # Verificar compatibilidad de tamaÃ±o de sector para L2ARC
        show_message "Verificando compatibilidad de sector para L2ARC..."
        local pool_sector_size=$(zpool get ashift "$pool_name" -H -o value 2>/dev/null | head -n 1)
        local l2arc_sector_size=$(sudo blockdev --getpbsz "/dev/$l2arc_device" 2>/dev/null)
        local l2arc_force=""
        
        if [ -n "$pool_sector_size" ] && [ -n "$l2arc_sector_size" ]; then
            local pool_bytes=$((2 ** pool_sector_size))
            if [ "$l2arc_sector_size" -ne "$pool_bytes" ]; then
                echo "   âš ï¸  Pool: ${pool_bytes} bytes | L2ARC: ${l2arc_sector_size} bytes"
                if confirm "Â¿Forzar L2ARC?"; then
                    l2arc_force="-f"
                else
                    show_message "ConfiguraciÃ³n cancelada"
                    return 1
                fi
            fi
        fi
        
        show_message "Agregando L2ARC al pool..."
        if ! sudo zpool add $l2arc_force "$pool_name" cache "/dev/$l2arc_device"; then
            show_error "âŒ Error configurando L2ARC"
            return 1
        fi
        
        # Configurar SLOG
        show_message "Preparando /dev/$slog_device para SLOG..."
        clean_disk "$slog_device"
        
        # Verificar compatibilidad de tamaÃ±o de sector para SLOG
        show_message "Verificando compatibilidad de sector para SLOG..."
        local slog_sector_size=$(sudo blockdev --getpbsz "/dev/$slog_device" 2>/dev/null)
        local slog_force=""
        
        if [ -n "$pool_sector_size" ] && [ -n "$slog_sector_size" ]; then
            local pool_bytes=$((2 ** pool_sector_size))
            if [ "$slog_sector_size" -ne "$pool_bytes" ]; then
                echo "   âš ï¸  Pool: ${pool_bytes} bytes | SLOG: ${slog_sector_size} bytes"
                if confirm "Â¿Forzar SLOG? (RIESGOSO para integridad)"; then
                    slog_force="-f"
                else
                    show_message "ConfiguraciÃ³n cancelada"
                    return 1
                fi
            fi
        fi
        
        show_message "Agregando SLOG al pool..."
        if ! sudo zpool add $slog_force "$pool_name" log "/dev/$slog_device"; then
            show_error "âŒ Error configurando SLOG"
            return 1
        fi
        
        show_message "âœ… L2ARC y SLOG configurados exitosamente"
        echo ""
        show_message "ğŸ“Š CONFIGURACIÃ“N COMPLETADA:"
        echo "   Pool: $pool_name"
        echo "   L2ARC: /dev/$l2arc_device (mejora lecturas frecuentes)"
        echo "   SLOG: /dev/$slog_device (mejora escrituras sÃ­ncronas)"
        return 0
    else
        show_message "ConfiguraciÃ³n cancelada"
        return 1
    fi
}

# FunciÃ³n para configurar particiones L2ARC + SLOG en un dispositivo
setup_partitioned_cache() {
    local pool_name="$1"
    shift
    local available_devices=("$@")
    
    show_title "ConfiguraciÃ³n de L2ARC + SLOG Particionado para '$pool_name'"
    echo ""
    echo "ğŸ’¡ CONFIGURACIÃ“N PARTICIONADA:"
    echo "   Esta opciÃ³n crea particiones en un solo dispositivo:"
    echo "   â€¢ 80% del espacio para L2ARC (cache de lectura)"
    echo "   â€¢ 20% del espacio para SLOG (log de escritura)"
    echo ""
    echo "âš ï¸  RECOMENDACIONES:"
    echo "   â€¢ Usar solo con SSD de alta calidad (preferiblemente NVMe)"
    echo "   â€¢ El dispositivo debe tener al menos 8GB de espacio"
    echo "   â€¢ No recomendado para HDD o dispositivos USB"
    echo ""
    
    # Mostrar dispositivos recomendados
    echo "ğŸ’¾ DISPOSITIVOS DISPONIBLES:"
    for i in "${!available_devices[@]}"; do
        local device_info="${available_devices[$i]}"
        local device_name=$(echo "$device_info" | cut -d: -f1)
        local usage_status=$(echo "$device_info" | cut -d: -f2)
        local size=$(lsblk -dpno SIZE "/dev/$device_name" | tr -d ' ')
        local size_bytes=$(lsblk -dpno SIZE "/dev/$device_name" --bytes)
        local model=$(lsblk -dpno MODEL "/dev/$device_name" | tr -d ' ')
        local recommendation=""
        
        if [[ "$device_name" == nvme* ]]; then
            recommendation=" â­ EXCELENTE"
        elif lsblk -dpno ROTA "/dev/$device_name" | grep -q "0"; then
            if [ "$size_bytes" -gt 8589934592 ]; then  # 8GB
                recommendation=" âœ… ADECUADO"
            else
                recommendation=" âš ï¸  PEQUEÃ‘O (mÃ­n 8GB recomendado)"
            fi
        else
            recommendation=" âŒ NO RECOMENDADO (HDD)"
        fi
        
        if [ "$usage_status" = "DISPONIBLE" ]; then
            echo "  $((i+1)). $device_name - $size - $model$recommendation"
        else
            echo "  $((i+1)). $device_name - $size - $model$recommendation (EN USO: $usage_status)"
        fi
    done
    
    while true; do
        read -p "ğŸ‘‰ Selecciona dispositivo (1-${#available_devices[@]}): " device_choice
        if [[ "$device_choice" =~ ^[0-9]+$ ]] && [ "$device_choice" -ge 1 ] && [ "$device_choice" -le ${#available_devices[@]} ]; then
            local device_info="${available_devices[$((device_choice-1))]}"
            local selected_device=$(echo "$device_info" | cut -d: -f1)
            local usage_status=$(echo "$device_info" | cut -d: -f2)
            
            # Verificar si el dispositivo estÃ¡ en uso
            if [ "$usage_status" != "DISPONIBLE" ]; then
                echo ""
                echo "âš ï¸  ADVERTENCIA: El dispositivo $selected_device estÃ¡ EN USO"
                echo "   ğŸ“ Uso actual: $usage_status"
                echo "   ğŸ’€ Crear particiones DESTRUIRÃ todos los datos"
                echo ""
                if ! confirm "Â¿EstÃ¡s SEGURO de que quieres limpiar y particionar $selected_device?"; then
                    echo "Selecciona otro dispositivo..."
                    continue
                fi
            fi
            break
        else
            echo "âŒ SelecciÃ³n invÃ¡lida. Usa nÃºmeros del 1 al ${#available_devices[@]}."
        fi
    done
    
    # Verificar si el dispositivo es adecuado
    local size_bytes=$(lsblk -dpno SIZE "/dev/$selected_device" --bytes)
    if [ "$size_bytes" -lt 8589934592 ]; then  # 8GB
        show_warning "âš ï¸  El dispositivo es menor a 8GB, puede no ser Ã³ptimo"
        if ! confirm "Â¿Continuar de todas formas?"; then
            show_message "ConfiguraciÃ³n cancelada"
            return 0
        fi
    fi
    
    if ! [[ "$selected_device" == nvme* ]] && lsblk -dpno ROTA "/dev/$selected_device" | grep -q "1"; then
        show_warning "âš ï¸  ADVERTENCIA: El dispositivo seleccionado es un HDD"
        show_warning "   No es recomendado usar HDD para particionado L2ARC+SLOG"
        if ! confirm "Â¿EstÃ¡s seguro de continuar?"; then
            show_message "ConfiguraciÃ³n cancelada"
            return 0
        fi
    fi
    
    # Mostrar resumen
    local size_gb=$(( size_bytes / 1024 / 1024 / 1024 ))
    local l2arc_size=$(( size_gb * 80 / 100 ))
    local slog_size=$(( size_gb * 20 / 100 ))
    
    echo ""
    show_message "ğŸ“Š CONFIGURACIÃ“N PLANIFICADA:"
    echo "   Dispositivo: /dev/$selected_device ($size_gb GB)"
    echo "   ParticiÃ³n L2ARC: ~${l2arc_size}GB (80%)"
    echo "   ParticiÃ³n SLOG: ~${slog_size}GB (20%)"
    echo ""
    
    show_warning "âš ï¸  Esto destruirÃ¡ todos los datos en /dev/$selected_device"
    if confirm "Â¿Continuar con el particionado?"; then
        echo ""
        show_message "ğŸ” VERIFICACIÃ“N PREVIA DEL POOL:"
        echo "   Pool: $pool_name"
        echo "   Estado: $(zpool get health "$pool_name" -H -o value 2>/dev/null || echo "DESCONOCIDO")"
        echo "   VersiÃ³n: $(zpool get version "$pool_name" -H -o value 2>/dev/null || echo "DESCONOCIDA")"
        echo ""
        
        show_message "Preparando dispositivo /dev/$selected_device..."
        
        # Limpiar dispositivo
        if ! clean_disk "$selected_device"; then
            show_error "âŒ Error limpiando el dispositivo"
            show_message "Regresando al menÃº anterior..."
            return 1
        fi
        
        # Crear tabla de particiones GPT
        show_message "Creando tabla de particiones..."
        if ! sudo parted "/dev/$selected_device" --script mklabel gpt; then
            show_error "âŒ Error creando tabla de particiones"
            show_message "Regresando al menÃº anterior..."
            return 1
        fi
        
        # Crear particiÃ³n L2ARC (80%)
        show_message "Creando particiÃ³n L2ARC (80% del espacio)..."
        if ! sudo parted "/dev/$selected_device" --script mkpart l2arc ext4 1MiB 80%; then
            show_error "âŒ Error creando particiÃ³n L2ARC"
            show_message "Regresando al menÃº anterior..."
            return 1
        fi
        
        # Crear particiÃ³n SLOG (20%)
        show_message "Creando particiÃ³n SLOG (20% del espacio)..."
        if ! sudo parted "/dev/$selected_device" --script mkpart slog ext4 80% 100%; then
            show_error "âŒ Error creando particiÃ³n SLOG"
            show_message "Regresando al menÃº anterior..."
            return 1
        fi
        
        # Esperar que el kernel reconozca las particiones
        sudo partprobe "/dev/$selected_device"
        sleep 3
        
        # Determinar nombres de particiones
        local l2arc_partition="${selected_device}1"
        local slog_partition="${selected_device}2"
        
        # Si es NVMe, las particiones tienen formato diferente
        if [[ "$selected_device" == nvme* ]]; then
            l2arc_partition="${selected_device}p1"
            slog_partition="${selected_device}p2"
        fi
        
        # Verificar que las particiones existen
        if [ ! -b "/dev/$l2arc_partition" ] || [ ! -b "/dev/$slog_partition" ]; then
            show_error "âŒ Error: Las particiones no se crearon correctamente"
            show_message "Regresando al menÃº anterior..."
            return 1
        fi
        
        # Agregar L2ARC y SLOG
        echo ""
        show_message "ğŸ”§ Agregando L2ARC y SLOG al pool en un solo comando..."
        echo "   ğŸ’¡ Cache devices no requieren compatibilidad de ashift"
        echo "   Comando: zpool add '$pool_name' cache '/dev/$l2arc_partition' log '/dev/$slog_partition'"
        
        # Verificar que ambas particiones existen y son accesibles
        if [ ! -b "/dev/$l2arc_partition" ]; then
            show_error "âŒ ParticiÃ³n L2ARC /dev/$l2arc_partition no existe"
            return 1
        fi
        
        if [ -n "$pool_sector_size" ] && [ -n "$cache_sector_size" ]; then
            local pool_bytes=$((2 ** pool_sector_size))
            echo "   Pool sector size: ${pool_bytes} bytes (ashift=$pool_sector_size)"
            echo "   Cache sector size: ${cache_sector_size} bytes"
            
            if [ "$cache_sector_size" -ne "$pool_bytes" ]; then
                echo ""
                show_error "âš ï¸  INCOMPATIBILIDAD DE TAMAÃ‘O DE SECTOR:"
                echo "   ğŸ“Š Pool '$pool_name': ${pool_bytes} bytes"
                echo "   ğŸ’¾ Cache device: ${cache_sector_size} bytes"
                echo ""
                show_message "[INFO] ğŸ” CAUSA DEL PROBLEMA:"
                echo "   El pool fue creado con ashift=$pool_sector_size"
                if [ "$pool_sector_size" -eq 0 ]; then
                    echo "   âš ï¸  ashift=0 indica pool creado sin configuraciÃ³n Ã³ptima"
                    echo "   ğŸ’¡ Para nuevos pools, el script ahora usa ashift automÃ¡tico"
                    echo "   ï¿½ RecomendaciÃ³n: Recrear pool con ashift=12 para compatibilidad"
                fi
                echo ""
                echo "ï¿½ğŸ’¡ OPCIONES DISPONIBLES:"
                echo "   1. Continuar SIN cache (recomendado)"
                echo "   2. Forzar adiciÃ³n con -f (puede causar problemas)"
                echo "   3. Usar un dispositivo con sector compatible"
                echo "   4. Recrear pool con ashift correcto (Ã³ptimo a largo plazo)"
                echo ""
                
                read -p "ğŸ‘‰ Selecciona opciÃ³n (1-3): " sector_choice
                case $sector_choice in
                    1)
                        show_message "ConfiguraciÃ³n cancelada - pool funcionarÃ¡ sin cache"
                        return 1
                        ;;
                    2)
                        show_warning "âš ï¸  Usando -f para forzar la adiciÃ³n..."
                        local force_flag="-f"
                        ;;
                    3)
                        show_message "ConfiguraciÃ³n cancelada - selecciona otro dispositivo"
                        return 1
                        ;;
                    *)
                        show_message "OpciÃ³n invÃ¡lida - cancelando configuraciÃ³n"
                        return 1
                        ;;
                esac
            else
                echo "   âœ… TamaÃ±os de sector compatibles"
                local force_flag=""
            fi
        else
            echo "   âš ï¸  No se pudo verificar tamaÃ±o de sector completamente"
            echo "   Intentando adiciÃ³n normal..."
            local force_flag=""
        fi
        
        # Agregar L2ARC
        echo ""
        show_message "ğŸ”§ Agregando L2ARC y SLOG al pool en un solo comando..."
        echo "   Comando: zpool add $force_flag '$pool_name' cache '/dev/$l2arc_partition' log '/dev/$slog_partition'"
        
        # Verificar que ambas particiones existen y son accesibles
        if [ ! -b "/dev/$l2arc_partition" ]; then
            show_error "âŒ ParticiÃ³n L2ARC /dev/$l2arc_partition no existe"
            return 1
        fi
        
        if [ ! -b "/dev/$slog_partition" ]; then
            show_error "âŒ ParticiÃ³n SLOG /dev/$slog_partition no existe"
            return 1
        fi
        
        # Mostrar informaciÃ³n de ambas particiones antes de agregar
        echo "   ParticiÃ³n L2ARC: $(lsblk -no SIZE,FSTYPE "/dev/$l2arc_partition" 2>/dev/null || echo "sin formato")"
        echo "   ParticiÃ³n SLOG: $(lsblk -no SIZE,FSTYPE "/dev/$slog_partition" 2>/dev/null || echo "sin formato")"
        
        if sudo zpool add "$pool_name" cache "/dev/$l2arc_partition" log "/dev/$slog_partition" 2>&1; then
            show_message "âœ… L2ARC y SLOG agregados exitosamente en un solo comando"
            echo ""
            show_message "ğŸ“Š CONFIGURACIÃ“N COMPLETADA:"
            echo "   Pool: $pool_name"
            echo "   Dispositivo base: /dev/$selected_device"
            echo "   L2ARC: /dev/$l2arc_partition (~${l2arc_size}GB)"
            echo "   SLOG: /dev/$slog_partition (~${slog_size}GB)"
            echo "   FunciÃ³n: Cache de lectura + Log de escritura optimizados"
            echo "   ğŸ’¡ Ambos dispositivos agregados en comando Ãºnico"
            return 0
        else
            local exit_code=$?
            show_error "âŒ Error agregando L2ARC y SLOG (cÃ³digo: $exit_code)"
            echo ""
            echo "ï¿½ INFORMACIÃ“N DE DIAGNÃ“STICO:"
            echo "   â€¢ Pool status:"
            zpool status "$pool_name" | head -n 10
            echo "   â€¢ Dispositivo L2ARC:"
            lsblk "/dev/$l2arc_partition" 2>/dev/null || echo "     No se puede leer la particiÃ³n"
            echo "   â€¢ Dispositivo SLOG:"
            lsblk "/dev/$slog_partition" 2>/dev/null || echo "     No se puede leer la particiÃ³n"
            echo "   â€¢ Posibles causas:"
            echo "     - ParticiÃ³n ya en uso"
            echo "     - Permisos insuficientes"
            echo "     - Pool en estado degradado"
            echo "   ğŸ’¡ Cache devices no requieren compatibilidad de ashift"
            show_message "Regresando al menÃº anterior..."
            return 1
        fi
    else
        show_message "ConfiguraciÃ³n cancelada"
        return 1
    fi
}

# FunciÃ³n para crear snapshots iniciales
create_initial_snapshots() {
        
        if sudo zpool add $force_flag "$pool_name" log "/dev/$slog_partition" 2>&1; then
            show_message "âœ… SLOG agregado exitosamente"
        else
            local exit_code=$?
            show_error "âŒ Error agregando SLOG (cÃ³digo: $exit_code)"
            echo ""
            echo "ï¿½ INFORMACIÃ“N DE DIAGNÃ“STICO:"
            echo "   â€¢ Pool status:"
            zpool status "$pool_name" | head -n 10
            echo "   â€¢ Dispositivo SLOG:"
            lsblk "/dev/$slog_partition" 2>/dev/null || echo "     No se puede leer la particiÃ³n"
            echo "   â€¢ Posibles causas:"
            echo "     - Sector size incompatible"
            echo "     - ParticiÃ³n ya en uso"
            echo "     - Permisos insuficientes"
            echo "     - Pool en estado degradado"
            show_message "Regresando al menÃº anterior..."
            return 1
        fi
}

# FunciÃ³n para crear snapshots iniciales
create_initial_snapshots() {
    local pool_name="$1"
    
    show_title "CreaciÃ³n de Snapshots Iniciales"
    echo ""
    echo "ğŸ’¡ Los snapshots son copias instantÃ¡neas de datasets que no ocupan espacio inicial."
    echo "   Son Ãºtiles para:"
    echo "   â€¢ Backups rÃ¡pidos antes de cambios importantes"
    echo "   â€¢ RestauraciÃ³n rÃ¡pida a estados anteriores"
    echo "   â€¢ Bases para replicaciÃ³n de datos"
    echo ""
    
    # Obtener datasets del pool
    local datasets=$(zfs list -H -o name -r "$pool_name" 2>/dev/null)
    
    if [ -n "$datasets" ]; then
        echo "ğŸ“ Datasets disponibles para snapshot:"
        for dataset in $datasets; do
            echo "  â€¢ $dataset"
        done
        echo ""
        
        if confirm "Â¿Crear snapshot inicial de todos los datasets?"; then
            local timestamp=$(date +"%Y%m%d-%H%M%S")
            local snapshot_name="inicial-$timestamp"
            
            for dataset in $datasets; do
                show_message "Creando snapshot: $dataset@$snapshot_name"
                if sudo zfs snapshot "$dataset@$snapshot_name"; then
                    show_message "âœ… Snapshot creado: $dataset@$snapshot_name"
                else
                    show_error "âŒ Error creando snapshot de $dataset"
                fi
            done
            
            echo ""
            show_message "ğŸ“Š Snapshots creados:"
            sudo zfs list -t snapshot -o name,used,creation | grep "@$snapshot_name"
        fi
    else
        show_warning "No se encontraron datasets en el pool '$pool_name'"
    fi
}

# FunciÃ³n para mostrar estado completo del pool
show_pool_status() {
    local pool_name="$1"
    
    # Debugging para identificar el problema
    if [ -z "$pool_name" ]; then
        show_error "Error crÃ­tico: show_pool_status llamado sin parÃ¡metro de pool_name"
        echo "InformaciÃ³n de debug:"
        echo "  ParÃ¡metros recibidos: '$@'"
        echo "  NÃºmero de parÃ¡metros: $#"
        echo "  Variable global POOL_NAME: '$POOL_NAME'"
        echo "  Pools disponibles: $(zpool list -H -o name 2>/dev/null | tr '\n' ' ')"
        read -p "Presiona Enter para continuar..."
        return 1
    fi
    
    show_title "Estado Completo del Pool '$pool_name'"
    echo ""
    
    # Estado del pool
    echo "ğŸŠ ESTADO DEL POOL:"
    sudo zpool status "$pool_name"
    echo ""
    
    # InformaciÃ³n del pool
    echo "ğŸ“Š INFORMACIÃ“N DEL POOL:"
    sudo zpool list "$pool_name"
    echo ""
    
    # Datasets
    echo "ğŸ“ DATASETS:"
    sudo zfs list -r "$pool_name"
    echo ""
    
    # Snapshots (si existen)
    local snapshots=$(zfs list -t snapshot -H -o name 2>/dev/null | grep "^$pool_name")
    if [ -n "$snapshots" ]; then
        echo "ğŸ“¸ SNAPSHOTS:"
        sudo zfs list -t snapshot -r "$pool_name"
        echo ""
    fi
    
    # Propiedades del pool
    echo "âš™ï¸  PROPIEDADES PRINCIPALES:"
    echo "ARC Max: $(cat /sys/module/zfs/parameters/zfs_arc_max 2>/dev/null || echo 'No configurado')"
    sudo zpool get all "$pool_name" | grep -E "ashift|feature|autoreplace|failmode"
}

# Ejecutar funciÃ³n principal
main "$@"